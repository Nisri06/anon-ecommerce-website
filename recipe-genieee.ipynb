{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11299040,"sourceType":"datasetVersion","datasetId":7065754}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nithyasrikumaravelu/recipe-genieee?scriptVersionId=233091175\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-09T06:16:46.948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pandas numpy torch transformers datasets accelerate sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T06:14:25.163693Z","iopub.execute_input":"2025-04-10T06:14:25.163894Z","iopub.status.idle":"2025-04-10T06:14:29.51718Z","shell.execute_reply.started":"2025-04-10T06:14:25.163875Z","shell.execute_reply":"2025-04-10T06:14:29.516141Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"*DATA PIPELINE* - Randomly generated recipe ingredients and steps, input - output pairs, saves(training dataset)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset, DatasetDict\nimport os\nimport json\nimport random\n\ndef create_recipe_dataset(sample_size=10000, seed=42):\n    \"\"\"\n    Create a compatible dataset for the flax-community/t5-recipe-generation model.\n    Uses sample data formatted to match the expected input/output structure.\n    \"\"\"\n    print(\"Creating dataset compatible with flax-community/t5-recipe-generation...\")\n    \n    # Set seeds for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    \n    # Load the tokenizer to understand the special tokens used by the model\n    tokenizer = AutoTokenizer.from_pretrained(\"flax-community/t5-recipe-generation\", use_fast=True)\n    \n    # Sample ingredients for synthetic data\n    common_ingredients = [\n        \"chicken\", \"beef\", \"pork\", \"salmon\", \"tuna\", \"shrimp\", \"lamb\",\n        \"pasta\", \"rice\", \"potatoes\", \"bread\", \"flour\", \"oats\", \"quinoa\",\n        \"onion\", \"garlic\", \"tomatoes\", \"bell peppers\", \"carrots\", \"broccoli\", \n        \"spinach\", \"lettuce\", \"mushrooms\", \"zucchini\", \"eggplant\",\n        \"butter\", \"olive oil\", \"vegetable oil\", \"coconut oil\", \n        \"salt\", \"pepper\", \"oregano\", \"basil\", \"thyme\", \"rosemary\", \"cumin\",\n        \"milk\", \"cream\", \"cheese\", \"yogurt\", \"eggs\", \"mayonnaise\",\n        \"sugar\", \"brown sugar\", \"honey\", \"maple syrup\",\n        \"beans\", \"lentils\", \"chickpeas\", \"tofu\", \"tempeh\",\n        \"chocolate\", \"vanilla extract\", \"cinnamon\", \"nutmeg\"\n    ]\n    \n    # Create directory for generated data\n    os.makedirs(\"synthetic_recipe_data\", exist_ok=True)\n    \n    # Generate synthetic recipes\n    recipes = []\n    \n    print(f\"Generating {sample_size} synthetic recipes...\")\n    for i in range(sample_size):\n        # Generate random recipe properties\n        num_ingredients = random.randint(4, 10)\n        ingredients = random.sample(common_ingredients, num_ingredients)\n        \n        # Create recipe title\n        main_ingredient = random.choice(ingredients)\n        cooking_methods = [\"Roasted\", \"Grilled\", \"Baked\", \"Fried\", \"Steamed\", \"Sautéed\", \"Slow-cooked\"]\n        dish_types = [\"Casserole\", \"Soup\", \"Stew\", \"Salad\", \"Pasta\", \"Curry\", \"Stir-fry\", \"Sandwich\"]\n        title = f\"{random.choice(cooking_methods)} {main_ingredient.capitalize()} {random.choice(dish_types)}\"\n        \n        # Create recipe directions\n        directions = [\n            f\"Prepare all the ingredients\",\n            f\"Heat {random.choice(['oven', 'pan', 'pot', 'skillet'])} to {random.randint(300, 450)} degrees\",\n            f\"Mix {ingredients[0]} and {ingredients[1]} together\",\n            f\"Add {', '.join(ingredients[2:4])} and cook for {random.randint(5, 30)} minutes\",\n            f\"Stir in {', '.join(ingredients[4:])}\",\n            f\"Season with salt and pepper to taste\",\n            f\"Serve hot\"\n        ]\n        \n        # Create formatted ingredient list for model input\n        input_ingredients = \", \".join(ingredients)\n        \n        # Format the output text with special tokens\n        output_text = (\n            f\"title: {title} <section> \"\n            f\"ingredients: {' <sep> '.join(ingredients)} <section> \"\n            f\"directions: {' <sep> '.join(directions)}\"\n        )\n        \n        recipes.append({\n            \"input_text\": f\"items: {input_ingredients}\",\n            \"output_text\": output_text\n        })\n    \n    # Save raw synthetic data\n    with open(\"synthetic_recipe_data/synthetic_recipes.json\", 'w') as f:\n        json.dump(recipes, f)\n    \n    # Create dataset splits\n    df = pd.DataFrame(recipes)\n    \n    # Split into train/validation/test (80/10/10)\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n    train_size = int(0.8 * len(df))\n    val_size = int(0.1 * len(df))\n    \n    train_df = df[:train_size]\n    val_df = df[train_size:train_size+val_size]\n    test_df = df[train_size+val_size:]\n    \n    # Convert to HuggingFace datasets\n    dataset_dict = DatasetDict({\n        'train': Dataset.from_pandas(train_df),\n        'validation': Dataset.from_pandas(val_df),\n        'test': Dataset.from_pandas(test_df)\n    })\n    \n    # Save dataset\n    dataset_dict.save_to_disk(\"recipe_dataset\")\n    print(f\"Dataset created with {len(df)} recipes\")\n    print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n    \n    # Display a few examples\n    print(\"\\nExample recipes:\")\n    for i in range(3):\n        idx = random.randint(0, len(df) - 1)\n        print(f\"\\nInput: {df.iloc[idx]['input_text']}\")\n        print(f\"Output: {df.iloc[idx]['output_text']}\")\n    \n    return dataset_dict\n\nif __name__ == \"__main__\":\n    create_recipe_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:32:34.786585Z","iopub.execute_input":"2025-04-10T10:32:34.787036Z","iopub.status.idle":"2025-04-10T10:32:36.90237Z","shell.execute_reply.started":"2025-04-10T10:32:34.786998Z","shell.execute_reply":"2025-04-10T10:32:36.901713Z"}},"outputs":[{"name":"stdout","text":"Creating dataset compatible with flax-community/t5-recipe-generation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a146aa0820c243beb7fd5d63c0b69cd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a05ff5add5f4e45b72ee523b9f367a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5195c54771c24be281ddc03d0a2f3f5f"}},"metadata":{}},{"name":"stdout","text":"Generating 10000 synthetic recipes...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58cb869f823e4e1a8bc1b6fc109edd39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b4bf3a58cd141b3954c0320022610ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75adc07bbe640f19a34e69fba915088"}},"metadata":{}},{"name":"stdout","text":"Dataset created with 10000 recipes\nTrain: 8000, Validation: 1000, Test: 1000\n\nExample recipes:\n\nInput: items: salmon, nutmeg, milk, flour, cumin, bread, pasta, chicken\nOutput: title: Slow-cooked Flour Sandwich <section> ingredients: salmon <sep> nutmeg <sep> milk <sep> flour <sep> cumin <sep> bread <sep> pasta <sep> chicken <section> directions: Prepare all the ingredients <sep> Heat oven to 385 degrees <sep> Mix salmon and nutmeg together <sep> Add milk, flour and cook for 8 minutes <sep> Stir in cumin, bread, pasta, chicken <sep> Season with salt and pepper to taste <sep> Serve hot\n\nInput: items: flour, lamb, oregano, potatoes, pork, carrots, rosemary\nOutput: title: Slow-cooked Pork Stir-fry <section> ingredients: flour <sep> lamb <sep> oregano <sep> potatoes <sep> pork <sep> carrots <sep> rosemary <section> directions: Prepare all the ingredients <sep> Heat oven to 376 degrees <sep> Mix flour and lamb together <sep> Add oregano, potatoes and cook for 8 minutes <sep> Stir in pork, carrots, rosemary <sep> Season with salt and pepper to taste <sep> Serve hot\n\nInput: items: potatoes, milk, rosemary, thyme, pepper, flour, chocolate\nOutput: title: Fried Thyme Soup <section> ingredients: potatoes <sep> milk <sep> rosemary <sep> thyme <sep> pepper <sep> flour <sep> chocolate <section> directions: Prepare all the ingredients <sep> Heat pan to 310 degrees <sep> Mix potatoes and milk together <sep> Add rosemary, thyme and cook for 14 minutes <sep> Stir in pepper, flour, chocolate <sep> Season with salt and pepper to taste <sep> Serve hot\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, T5TokenizerFast\nfrom datasets import load_from_disk\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef preprocess_recipe_data(model_name=\"t5-base\", max_input_length=256, max_output_length=512):\n    \"\"\"\n    Preprocess recipe dataset for sequence-to-sequence training with T5.\n    \"\"\"\n    print(\"Preprocessing recipe dataset...\")\n    \n    # Load dataset\n    dataset = load_from_disk(\"recipe_dataset\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Add special tokens for recipe formatting\n    special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    print(f\"Added special tokens to the tokenizer\")\n    \n    def preprocess_function(examples):\n        # Tokenize inputs\n        model_inputs = tokenizer(\n            examples[\"input_text\"],\n            max_length=max_input_length,\n            padding=\"max_length\",\n            truncation=True\n        )\n        \n        # Tokenize outputs\n        labels = tokenizer(\n            examples[\"output_text\"],\n            max_length=max_output_length,\n            padding=\"max_length\",\n            truncation=True\n        )\n        \n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        \n        # Replace padding token id with -100 for loss calculation\n        for i in range(len(model_inputs[\"labels\"])):\n            model_inputs[\"labels\"][i] = [\n                -100 if token == tokenizer.pad_token_id else token \n                for token in model_inputs[\"labels\"][i]\n            ]\n        \n        return model_inputs\n    \n    # Apply preprocessing to all splits\n    processed_dataset = dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=dataset[\"train\"].column_names\n    )\n    \n    # Save processed dataset\n    processed_dataset.save_to_disk(\"preprocessed_recipe_dataset\")\n    print(f\"Preprocessed dataset saved to 'preprocessed_recipe_dataset'\")\n    \n    # Create PyTorch dataloaders\n    def collate_fn(batch):\n        input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n        attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch])\n        labels = torch.tensor([item[\"labels\"] for item in batch])\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n    \n    train_dataloader = DataLoader(\n        processed_dataset[\"train\"], \n        batch_size=8, \n        shuffle=True, \n        collate_fn=collate_fn\n    )\n    \n    eval_dataloader = DataLoader(\n        processed_dataset[\"validation\"], \n        batch_size=8, \n        collate_fn=collate_fn\n    )\n    \n    print(f\"Created dataloaders with batch size 8\")\n    print(f\"Training batches: {len(train_dataloader)}, Validation batches: {len(eval_dataloader)}\")\n    \n    return processed_dataset, tokenizer, train_dataloader, eval_dataloader\n\nif __name__ == \"__main__\":\n    processed_dataset, tokenizer, _, _ = preprocess_recipe_data()\n    print(f\"Processed dataset created with {len(processed_dataset['train'])} training examples\")\n    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n    \n    # Sample preprocessing results\n    sample = processed_dataset[\"train\"][0]\n    print(\"\\nSample preprocessed example:\")\n    print(f\"Input IDs (first 10): {sample['input_ids'][:10]}\")\n    print(f\"Attention mask (first 10): {sample['attention_mask'][:10]}\")\n    print(f\"Labels (first 10): {sample['labels'][:10]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:32:42.791764Z","iopub.execute_input":"2025-04-10T10:32:42.792089Z","iopub.status.idle":"2025-04-10T10:33:09.51908Z","shell.execute_reply.started":"2025-04-10T10:32:42.792065Z","shell.execute_reply":"2025-04-10T10:33:09.518236Z"}},"outputs":[{"name":"stdout","text":"Preprocessing recipe dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b27ccc0b814deb92bc461b7be56d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032512fa664049348e526dc9ed0832ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38347f04b32f4f2c9ab1d33f50a634ed"}},"metadata":{}},{"name":"stdout","text":"Added special tokens to the tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d9616eeefc443bb438caf8bd6014c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f34a130b7ddd4964b7d61ad0fea76684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d3c057e32cb4972ab0ece3cb0cfab08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61518f3471b6403e8b872a31be3c2e35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d7945e3e6d423e9b2ae1100aab085d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91463ffbdaa94c7da5be28649838e674"}},"metadata":{}},{"name":"stdout","text":"Preprocessed dataset saved to 'preprocessed_recipe_dataset'\nCreated dataloaders with batch size 8\nTraining batches: 1000, Validation batches: 125\nProcessed dataset created with 8000 training examples\nTokenizer vocabulary size: 32102\n\nSample preprocessed example:\nInput IDs (first 10): [1173, 10, 5240, 9, 6, 18684, 6, 21659, 6, 9177]\nAttention mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLabels (first 10): [2233, 10, 7745, 17, 721, 26, 10792, 1836, 26335, 32101]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class RecipePostProcessor:\n    def __init__(self, tokenizer):\n        \"\"\"\n        Initialize post-processor with tokenizer for handling special tokens.\n        Designed to work with the synthetic dataset format.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.special_tokens = tokenizer.all_special_tokens\n        self.tokens_map = {\n            \"<sep>\": \"--\",\n            \"<section>\": \"\\n\"\n        }\n    \n    def postprocess_text(self, generated_texts):\n        \"\"\"\n        Post-process generated recipe texts:\n        1. Remove special tokens except our custom <sep> and <section>\n        2. Replace mapped tokens with their human-readable versions\n        3. Format into structured recipes\n        \"\"\"\n        processed_recipes = []\n        \n        for text in generated_texts:\n            # Remove special tokens except our custom ones\n            for token in self.special_tokens:\n                if token not in self.tokens_map:\n                    text = text.replace(token, \"\")\n            \n            # Replace mapped tokens with readable versions\n            for k, v in self.tokens_map.items():\n                text = text.replace(k, v)\n            \n            # Format structured recipe\n            formatted_recipe = self._format_recipe(text)\n            processed_recipes.append(formatted_recipe)\n        \n        return processed_recipes\n    \n    def _format_recipe(self, text):\n        \"\"\"Format recipe text into structured dictionary\"\"\"\n        recipe_dict = {\"title\": \"\", \"ingredients\": [], \"directions\": []}\n        \n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            \n            if section.startswith(\"title:\"):\n                recipe_dict[\"title\"] = section.replace(\"title:\", \"\").strip().capitalize()\n            \n            elif section.startswith(\"ingredients:\"):\n                ingr_text = section.replace(\"ingredients:\", \"\").strip()\n                recipe_dict[\"ingredients\"] = [\n                    item.strip().capitalize() for item in ingr_text.split(\"--\") if item.strip()\n                ]\n            \n            elif section.startswith(\"directions:\"):\n                dir_text = section.replace(\"directions:\", \"\").strip()\n                recipe_dict[\"directions\"] = [\n                    item.strip().capitalize() for item in dir_text.split(\"--\") if item.strip()\n                ]\n        \n        return recipe_dict\n    \n    def format_for_display(self, recipe_dict):\n        \"\"\"Format recipe dictionary for display\"\"\"\n        display_text = f\"[TITLE]: {recipe_dict['title']}\\n\\n\"\n        \n        display_text += \"[INGREDIENTS]:\\n\"\n        for i, ingredient in enumerate(recipe_dict['ingredients']):\n            display_text += f\"  - {i+1}: {ingredient}\\n\"\n        \n        display_text += \"\\n[DIRECTIONS]:\\n\"\n        for i, step in enumerate(recipe_dict['directions']):\n            display_text += f\"  - {i+1}: {step}\\n\"\n        \n        return display_text\n    \n    def evaluate_recipe_quality(self, recipe_dict):\n        \"\"\"Evaluate recipe quality based on simple heuristics\"\"\"\n        score = 0\n        max_score = 100\n        \n        # Check title\n        if recipe_dict[\"title\"] and len(recipe_dict[\"title\"]) > 3:\n            score += 10\n        \n        # Check ingredients\n        if recipe_dict[\"ingredients\"]:\n            score += min(len(recipe_dict[\"ingredients\"]) * 5, 30)  # Up to 30 points for ingredients\n            \n            # Check for ingredient variety (rough estimate)\n            unique_words = set()\n            for ingredient in recipe_dict[\"ingredients\"]:\n                unique_words.update(ingredient.lower().split())\n            score += min(len(unique_words) * 2, 20)  # Up to 20 points for variety\n        \n        # Check directions\n        if recipe_dict[\"directions\"]:\n            score += min(len(recipe_dict[\"directions\"]) * 5, 30)  # Up to 30 points for directions\n            \n            # Check for detailed instructions (rough estimate by length)\n            total_length = sum(len(step) for step in recipe_dict[\"directions\"])\n            score += min(total_length // 50, 10)  # Up to 10 points for detail\n        \n        # Normalize to 100\n        normalized_score = min(score, max_score)\n        \n        return normalized_score\n\nif __name__ == \"__main__\":\n    # Example usage\n    from transformers import AutoTokenizer\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n    special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    \n    processor = RecipePostProcessor(tokenizer)\n    \n    sample_text = \"title: roasted chicken pasta <section> ingredients: chicken <sep> pasta <sep> garlic <sep> olive oil <sep> salt <sep> pepper <section> directions: prepare all the ingredients <sep> cook pasta <sep> season chicken <sep> roast chicken <sep> combine with pasta <sep> serve hot\"\n    \n    # Process a single text\n    processed = processor.postprocess_text([sample_text])[0]\n    display_text = processor.format_for_display(processed)\n    \n    print(\"Original text:\")\n    print(sample_text)\n    print(\"\\nProcessed recipe:\")\n    print(display_text)\n    \n    # Evaluate quality\n    quality_score = processor.evaluate_recipe_quality(processed)\n    print(f\"\\nRecipe quality score: {quality_score}/100\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:15.296185Z","iopub.execute_input":"2025-04-10T10:33:15.296493Z","iopub.status.idle":"2025-04-10T10:33:15.851617Z","shell.execute_reply.started":"2025-04-10T10:33:15.296466Z","shell.execute_reply":"2025-04-10T10:33:15.850874Z"}},"outputs":[{"name":"stdout","text":"Original text:\ntitle: roasted chicken pasta <section> ingredients: chicken <sep> pasta <sep> garlic <sep> olive oil <sep> salt <sep> pepper <section> directions: prepare all the ingredients <sep> cook pasta <sep> season chicken <sep> roast chicken <sep> combine with pasta <sep> serve hot\n\nProcessed recipe:\n[TITLE]: Roasted chicken pasta\n\n[INGREDIENTS]:\n  - 1: Chicken\n  - 2: Pasta\n  - 3: Garlic\n  - 4: Olive oil\n  - 5: Salt\n  - 6: Pepper\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Cook pasta\n  - 3: Season chicken\n  - 4: Roast chicken\n  - 5: Combine with pasta\n  - 6: Serve hot\n\n\nRecipe quality score: 85/100\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class RecipePostProcessor:\n    def __init__(self, tokenizer):\n        \"\"\"\n        Initialize post-processor with tokenizer for handling special tokens.\n        Includes allergen detection and additional recipe processing.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.special_tokens = tokenizer.all_special_tokens\n        self.tokens_map = {\n            \"<sep>\": \"--\",\n            \"<section>\": \"\\n\"\n        }\n        \n        # Allergen combinations to avoid\n        self.allergen_combinations = [\n            # Format: (ingredient1, ingredient2, reason)\n            (\"fish\", \"dairy\", \"Fish and dairy combinations can cause digestive issues for many people\"),\n            (\"shellfish\", \"dairy\", \"Shellfish and dairy combinations can trigger allergic reactions\"),\n            (\"fish\", \"yogurt\", \"Fish and yogurt may cause digestive issues\"),\n            (\"fish\", \"milk\", \"Fish and milk can cause adverse reactions in some individuals\"),\n            (\"fish\", \"curd\", \"Fish and curd combinations may cause allergic reactions\"),\n            (\"fish\", \"cheese\", \"Fish and cheese can trigger food sensitivities\"),\n            (\"peanuts\", \"gluten\", \"Peanuts and gluten can cause severe reactions in some people\"),\n            (\"shellfish\", \"mango\", \"Shellfish and mango can cause histamine reactions\"),\n            (\"strawberry\", \"chocolate\", \"Strawberry and chocolate may trigger migraine in sensitive individuals\")\n        ]\n    \n    def postprocess_text(self, generated_texts):\n        \"\"\"\n        Post-process generated recipe texts:\n        1. Remove special tokens except our custom ones\n        2. Replace mapped tokens with their human-readable versions\n        3. Format into structured recipes\n        4. Check for allergen combinations\n        \"\"\"\n        processed_recipes = []\n        \n        for text in generated_texts:\n            # Remove special tokens except our custom ones\n            for token in self.special_tokens:\n                if token not in self.tokens_map:\n                    text = text.replace(token, \"\")\n            \n            # Replace mapped tokens with readable versions\n            for k, v in self.tokens_map.items():\n                text = text.replace(k, v)\n            \n            # Format structured recipe\n            formatted_recipe = self._format_recipe(text)\n            \n            # Check for allergen combinations\n            allergen_warnings = self.check_allergens(formatted_recipe)\n            formatted_recipe[\"allergen_warnings\"] = allergen_warnings\n            \n            processed_recipes.append(formatted_recipe)\n        \n        return processed_recipes\n    \n    def _format_recipe(self, text):\n        \"\"\"Format recipe text into structured dictionary\"\"\"\n        recipe_dict = {\"title\": \"\", \"ingredients\": [], \"directions\": [], \"allergen_warnings\": []}\n        \n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            \n            if section.startswith(\"title:\"):\n                recipe_dict[\"title\"] = section.replace(\"title:\", \"\").strip().capitalize()\n            \n            elif section.startswith(\"ingredients:\"):\n                ingr_text = section.replace(\"ingredients:\", \"\").strip()\n                recipe_dict[\"ingredients\"] = [\n                    item.strip().capitalize() for item in ingr_text.split(\"--\") if item.strip()\n                ]\n            \n            elif section.startswith(\"directions:\"):\n                dir_text = section.replace(\"directions:\", \"\").strip()\n                recipe_dict[\"directions\"] = [\n                    item.strip().capitalize() for item in dir_text.split(\"--\") if item.strip()\n                ]\n        \n        return recipe_dict\n    \n    def check_allergens(self, recipe_dict):\n        \"\"\"Check for potentially problematic ingredient combinations\"\"\"\n        warnings = []\n        ingredients_lower = [ing.lower() for ing in recipe_dict[\"ingredients\"]]\n        \n        for ing1, ing2, reason in self.allergen_combinations:\n            # Check if both ingredients are present\n            has_ing1 = any(ing1 in ingredient for ingredient in ingredients_lower)\n            has_ing2 = any(ing2 in ingredient for ingredient in ingredients_lower)\n            \n            if has_ing1 and has_ing2:\n                warnings.append({\n                    \"ingredients\": (ing1, ing2),\n                    \"reason\": reason\n                })\n        \n        return warnings\n    \n    def filter_allergenic_recipes(self, recipes):\n        \"\"\"Filter out recipes with allergen combinations\"\"\"\n        safe_recipes = []\n        filtered_count = 0\n        \n        for recipe in recipes:\n            if not recipe.get(\"allergen_warnings\", []):\n                safe_recipes.append(recipe)\n            else:\n                filtered_count += 1\n        \n        print(f\"Filtered {filtered_count} recipes with allergen combinations\")\n        return safe_recipes\n    \n    def format_for_display(self, recipe_dict):\n        \"\"\"Format recipe dictionary for display\"\"\"\n        display_text = f\"[TITLE]: {recipe_dict['title']}\\n\\n\"\n        \n        display_text += \"[INGREDIENTS]:\\n\"\n        for i, ingredient in enumerate(recipe_dict['ingredients']):\n            display_text += f\"  - {i+1}: {ingredient}\\n\"\n        \n        display_text += \"\\n[DIRECTIONS]:\\n\"\n        for i, step in enumerate(recipe_dict['directions']):\n            display_text += f\"  - {i+1}: {step}\\n\"\n        \n        # Add allergen warnings if present\n        if recipe_dict.get(\"allergen_warnings\", []):\n            display_text += \"\\n[⚠️ ALLERGEN WARNINGS]:\\n\"\n            for i, warning in enumerate(recipe_dict[\"allergen_warnings\"]):\n                display_text += f\"  - Warning {i+1}: {warning['ingredients'][0]} and {warning['ingredients'][1]} - {warning['reason']}\\n\"\n        \n        return display_text\n    \n    def evaluate_recipe_quality(self, recipe_dict):\n        \"\"\"Evaluate recipe quality based on simple heuristics\"\"\"\n        score = 0\n        max_score = 100\n        \n        # Check title\n        if recipe_dict[\"title\"] and len(recipe_dict[\"title\"]) > 3:\n            score += 10\n        \n        # Check ingredients\n        if recipe_dict[\"ingredients\"]:\n            score += min(len(recipe_dict[\"ingredients\"]) * 5, 30)  # Up to 30 points for ingredients\n            \n            # Check for ingredient variety (rough estimate)\n            unique_words = set()\n            for ingredient in recipe_dict[\"ingredients\"]:\n                unique_words.update(ingredient.lower().split())\n            score += min(len(unique_words) * 2, 20)  # Up to 20 points for variety\n        \n        # Check directions\n        if recipe_dict[\"directions\"]:\n            score += min(len(recipe_dict[\"directions\"]) * 5, 30)  # Up to 30 points for directions\n            \n            # Check for detailed instructions (rough estimate by length)\n            total_length = sum(len(step) for step in recipe_dict[\"directions\"])\n            score += min(total_length // 50, 10)  # Up to 10 points for detail\n        \n        # Deduct points for allergen warnings\n        allergen_warnings = recipe_dict.get(\"allergen_warnings\", [])\n        score -= len(allergen_warnings) * 15  # Deduct 15 points per allergen warning\n        \n        # Normalize to 100 (but don't go below 0)\n        normalized_score = max(min(score, max_score), 0)\n        \n        return normalized_score\n\nif __name__ == \"__main__\":\n    # Example usage\n    from transformers import AutoTokenizer\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n    special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    \n    processor = RecipePostProcessor(tokenizer)\n    \n    # Example 1: Basic recipe\n    sample_text1 = \"title: roasted chicken pasta <section> ingredients: chicken <sep> pasta <sep> garlic <sep> olive oil <sep> salt <sep> pepper <section> directions: prepare all the ingredients <sep> cook pasta <sep> season chicken <sep> roast chicken <sep> combine with pasta <sep> serve hot\"\n    \n    # Example 2: Recipe with potential allergens (fish + cheese)\n    sample_text2 = \"title: tuna melt sandwich <section> ingredients: tuna <sep> mayonnaise <sep> cheese <sep> bread <sep> tomato <sep> onion <section> directions: mix tuna with mayonnaise <sep> place on bread <sep> top with cheese <sep> toast until cheese melts <sep> add tomato and onion <sep> serve warm\"\n    \n    # Example 3: Another recipe with different allergens (shellfish + dairy)\n    sample_text3 = \"title: creamy shrimp pasta <section> ingredients: shrimp <sep> pasta <sep> heavy cream <sep> parmesan cheese <sep> garlic <sep> butter <sep> parsley <section> directions: cook pasta <sep> sauté garlic in butter <sep> add shrimp and cook <sep> add heavy cream <sep> add cooked pasta <sep> sprinkle with parmesan <sep> garnish with parsley\"\n    \n    # Process all sample texts\n    print(\"Processing multiple recipes:\\n\")\n    all_sample_texts = [sample_text1, sample_text2, sample_text3]\n    processed_recipes = processor.postprocess_text(all_sample_texts)\n    \n    for i, recipe in enumerate(processed_recipes):\n        print(f\"\\n--- RECIPE {i+1} ---\")\n        display_text = processor.format_for_display(recipe)\n        print(display_text)\n        \n        # Evaluate quality\n        quality_score = processor.evaluate_recipe_quality(recipe)\n        print(f\"Recipe quality score: {quality_score}/100\")\n    \n    # Demonstrate filtering allergenic recipes\n    print(\"\\nFiltering allergenic recipes:\")\n    safe_recipes = processor.filter_allergenic_recipes(processed_recipes)\n    print(f\"Original recipes: {len(processed_recipes)}, Safe recipes: {len(safe_recipes)}\")\n    \n    # Display the safe recipes\n    print(\"\\nSafe recipes after filtering:\")\n    for i, recipe in enumerate(safe_recipes):\n        print(f\"\\n--- SAFE RECIPE {i+1} ---\")\n        print(processor.format_for_display(recipe))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:21.966957Z","iopub.execute_input":"2025-04-10T10:33:21.967269Z","iopub.status.idle":"2025-04-10T10:33:22.509309Z","shell.execute_reply.started":"2025-04-10T10:33:21.967246Z","shell.execute_reply":"2025-04-10T10:33:22.508393Z"}},"outputs":[{"name":"stdout","text":"Processing multiple recipes:\n\n\n--- RECIPE 1 ---\n[TITLE]: Roasted chicken pasta\n\n[INGREDIENTS]:\n  - 1: Chicken\n  - 2: Pasta\n  - 3: Garlic\n  - 4: Olive oil\n  - 5: Salt\n  - 6: Pepper\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Cook pasta\n  - 3: Season chicken\n  - 4: Roast chicken\n  - 5: Combine with pasta\n  - 6: Serve hot\n\nRecipe quality score: 85/100\n\n--- RECIPE 2 ---\n[TITLE]: Tuna melt sandwich\n\n[INGREDIENTS]:\n  - 1: Tuna\n  - 2: Mayonnaise\n  - 3: Cheese\n  - 4: Bread\n  - 5: Tomato\n  - 6: Onion\n\n[DIRECTIONS]:\n  - 1: Mix tuna with mayonnaise\n  - 2: Place on bread\n  - 3: Top with cheese\n  - 4: Toast until cheese melts\n  - 5: Add tomato and onion\n  - 6: Serve warm\n\nRecipe quality score: 84/100\n\n--- RECIPE 3 ---\n[TITLE]: Creamy shrimp pasta\n\n[INGREDIENTS]:\n  - 1: Shrimp\n  - 2: Pasta\n  - 3: Heavy cream\n  - 4: Parmesan cheese\n  - 5: Garlic\n  - 6: Butter\n  - 7: Parsley\n\n[DIRECTIONS]:\n  - 1: Cook pasta\n  - 2: Sauté garlic in butter\n  - 3: Add shrimp and cook\n  - 4: Add heavy cream\n  - 5: Add cooked pasta\n  - 6: Sprinkle with parmesan\n  - 7: Garnish with parsley\n\nRecipe quality score: 90/100\n\nFiltering allergenic recipes:\nFiltered 0 recipes with allergen combinations\nOriginal recipes: 3, Safe recipes: 3\n\nSafe recipes after filtering:\n\n--- SAFE RECIPE 1 ---\n[TITLE]: Roasted chicken pasta\n\n[INGREDIENTS]:\n  - 1: Chicken\n  - 2: Pasta\n  - 3: Garlic\n  - 4: Olive oil\n  - 5: Salt\n  - 6: Pepper\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Cook pasta\n  - 3: Season chicken\n  - 4: Roast chicken\n  - 5: Combine with pasta\n  - 6: Serve hot\n\n\n--- SAFE RECIPE 2 ---\n[TITLE]: Tuna melt sandwich\n\n[INGREDIENTS]:\n  - 1: Tuna\n  - 2: Mayonnaise\n  - 3: Cheese\n  - 4: Bread\n  - 5: Tomato\n  - 6: Onion\n\n[DIRECTIONS]:\n  - 1: Mix tuna with mayonnaise\n  - 2: Place on bread\n  - 3: Top with cheese\n  - 4: Toast until cheese melts\n  - 5: Add tomato and onion\n  - 6: Serve warm\n\n\n--- SAFE RECIPE 3 ---\n[TITLE]: Creamy shrimp pasta\n\n[INGREDIENTS]:\n  - 1: Shrimp\n  - 2: Pasta\n  - 3: Heavy cream\n  - 4: Parmesan cheese\n  - 5: Garlic\n  - 6: Butter\n  - 7: Parsley\n\n[DIRECTIONS]:\n  - 1: Cook pasta\n  - 2: Sauté garlic in butter\n  - 3: Add shrimp and cook\n  - 4: Add heavy cream\n  - 5: Add cooked pasta\n  - 6: Sprinkle with parmesan\n  - 7: Garnish with parsley\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport time\n\n# Allergen test class that uses our RecipePostProcessor\nclass AllergenDetectionTester:\n    def __init__(self, model_path=\"./recipe_model\"):\n        \"\"\"Initialize the tester with model and post-processor.\"\"\"\n        print(f\"Loading model and tokenizer from {model_path}...\")\n        try:\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n        except:\n            print(\"Model not found, using t5-base for demonstration\")\n            self.tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n            self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n            \n        # Add special tokens for recipe formatting if needed\n        special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n        self.tokenizer.add_special_tokens(special_tokens)\n        \n        # Initialize the post-processor\n        self.processor = RecipePostProcessor(self.tokenizer)\n        \n        # Optimize model for inference\n        self.model = self.model.eval()\n        if torch.cuda.is_available():\n            print(\"Using GPU for inference\")\n            self.model = self.model.cuda()\n        else:\n            print(\"Using CPU for inference\")\n            \n        # Default generation parameters\n        self.generation_kwargs = {\n            \"max_length\": 512,\n            \"min_length\": 64,\n            \"no_repeat_ngram_size\": 3,\n            \"num_beams\": 4,\n            \"temperature\": 0.8,\n            \"do_sample\": True,\n            \"top_p\": 0.95\n        }\n        \n        print(\"Tester initialized\")\n        \n    def generate_recipe(self, ingredients):\n        \"\"\"Generate a recipe from ingredients.\"\"\"\n        # Format input\n        if isinstance(ingredients, list):\n            ingredients = \", \".join(ingredients)\n            \n        input_text = f\"items: {ingredients}\"\n        \n        # For demonstration, we'll also create a mock recipe if model is t5-base\n        if \"t5-base\" in self.tokenizer.name_or_path:\n            print(\"Using mock recipe generation for demonstration\")\n            # Create a mock recipe that includes all ingredients\n            ingredients_list = [ing.strip() for ing in ingredients.split(\",\")]\n            mock_recipe = (\n                f\"title: {ingredients_list[0].capitalize()} Recipe <section> \"\n                f\"ingredients: {' <sep> '.join(ingredients_list)} <section> \"\n                f\"directions: mix ingredients <sep> cook <sep> serve\"\n            )\n            return mock_recipe\n            \n        # Tokenize\n        inputs = self.tokenizer(\n            input_text, \n            max_length=256, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        \n        # Move to GPU if available\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        \n        # Generate\n        with torch.no_grad():\n            output_ids = self.model.generate(\n                **inputs,\n                **self.generation_kwargs\n            )\n        \n        # Decode\n        generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=False)\n        return generated_text\n        \n    def test_allergen_combinations(self):\n        \"\"\"Test various allergen combinations.\"\"\"\n        test_combinations = [\n            \"fish, cheese, pasta, garlic, olive oil\",\n            \"shrimp, milk, pasta, garlic, butter\",\n            \"tuna, yogurt, onions, celery, mayonnaise\",\n            \"salmon, curd, lemon, dill, rice\",\n            \"chicken, rice, vegetables, soy sauce\",  # Non-allergenic control\n            \"shellfish, mango, coconut, lime, ginger\",\n            \"peanuts, wheat flour, sugar, eggs, butter\",\n            \"strawberry, chocolate, cream, sugar, vanilla\"\n        ]\n        \n        results = []\n        \n        print(\"\\n======= TESTING ALLERGEN COMBINATIONS =======\\n\")\n        \n        for ingredients in test_combinations:\n            print(f\"Testing ingredients: {ingredients}\")\n            \n            # Generate a recipe\n            recipe_text = self.generate_recipe(ingredients)\n            \n            # Post-process the recipe\n            processed_recipes = self.processor.postprocess_text([recipe_text])\n            recipe = processed_recipes[0]\n            \n            # Calculate quality score\n            quality_score = self.processor.evaluate_recipe_quality(recipe)\n            \n            # Format the recipe for display\n            formatted_recipe = self.processor.format_for_display(recipe)\n            \n            results.append({\n                \"ingredients\": ingredients,\n                \"recipe\": recipe,\n                \"formatted_text\": formatted_recipe,\n                \"quality_score\": quality_score,\n                \"allergen_warnings\": recipe.get(\"allergen_warnings\", [])\n            })\n            \n            # Print the result\n            print(f\"\\n{formatted_recipe}\")\n            print(f\"Quality Score: {quality_score}/100\")\n            print(\"-\" * 50)\n            \n        # Summary of allergen detections\n        print(\"\\n======= ALLERGEN DETECTION SUMMARY =======\\n\")\n        for result in results:\n            warnings = result.get(\"allergen_warnings\", [])\n            print(f\"Ingredients: {result['ingredients']}\")\n            print(f\"Allergen Warnings: {len(warnings)}\")\n            for warning in warnings:\n                print(f\"  - {warning['ingredients'][0]} + {warning['ingredients'][1]}: {warning['reason']}\")\n            print()\n            \n        # Count safe vs. unsafe recipes\n        safe_recipes = [r for r in results if not r.get(\"allergen_warnings\")]\n        print(f\"Total Recipes: {len(results)}\")\n        print(f\"Safe Recipes: {len(safe_recipes)}\")\n        print(f\"Unsafe Recipes: {len(results) - len(safe_recipes)}\")\n        \n        return results\n\n# Recipe Post-processor definition\nclass RecipePostProcessor:\n    def __init__(self, tokenizer):\n        \"\"\"\n        Initialize post-processor with tokenizer for handling special tokens.\n        Includes allergen detection and additional recipe processing.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.special_tokens = tokenizer.all_special_tokens\n        self.tokens_map = {\n            \"<sep>\": \"--\",\n            \"<section>\": \"\\n\"\n        }\n        \n        # Allergen combinations to avoid\n        self.allergen_combinations = [\n            # Format: (ingredient1, ingredient2, reason)\n            (\"fish\", \"dairy\", \"Fish and dairy combinations can cause digestive issues for many people\"),\n            (\"fish\", \"yogurt\", \"Fish and yogurt may cause digestive issues\"),\n            (\"fish\", \"milk\", \"Fish and milk can cause adverse reactions in some individuals\"),\n            (\"fish\", \"curd\", \"Fish and curd combinations may cause allergic reactions\"),\n            (\"fish\", \"cheese\", \"Fish and cheese can trigger food sensitivities\"),\n            (\"tuna\", \"cheese\", \"Tuna and cheese can trigger food sensitivities\"),\n            (\"salmon\", \"cheese\", \"Salmon and cheese can trigger food sensitivities\"),\n            (\"shellfish\", \"dairy\", \"Shellfish and dairy combinations can trigger allergic reactions\"),\n            (\"shellfish\", \"milk\", \"Shellfish and milk can trigger allergic reactions\"),\n            (\"shrimp\", \"milk\", \"Shrimp and milk can trigger allergic reactions\"),\n            (\"peanuts\", \"gluten\", \"Peanuts and gluten can cause severe reactions in some people\"),\n            (\"peanuts\", \"wheat\", \"Peanuts and wheat can cause severe reactions in some people\"),\n            (\"shellfish\", \"mango\", \"Shellfish and mango can cause histamine reactions\"),\n            (\"strawberry\", \"chocolate\", \"Strawberry and chocolate may trigger migraine in sensitive individuals\")\n        ]\n    \n    def postprocess_text(self, generated_texts):\n        \"\"\"\n        Post-process generated recipe texts:\n        1. Remove special tokens except our custom ones\n        2. Replace mapped tokens with their human-readable versions\n        3. Format into structured recipes\n        4. Check for allergen combinations\n        \"\"\"\n        processed_recipes = []\n        \n        for text in generated_texts:\n            # Remove special tokens except our custom ones\n            for token in self.special_tokens:\n                if token not in self.tokens_map:\n                    text = text.replace(token, \"\")\n            \n            # Replace mapped tokens with readable versions\n            for k, v in self.tokens_map.items():\n                text = text.replace(k, v)\n            \n            # Format structured recipe\n            formatted_recipe = self._format_recipe(text)\n            \n            # Check for allergen combinations\n            allergen_warnings = self.check_allergens(formatted_recipe)\n            formatted_recipe[\"allergen_warnings\"] = allergen_warnings\n            \n            processed_recipes.append(formatted_recipe)\n        \n        return processed_recipes\n    \n    def _format_recipe(self, text):\n        \"\"\"Format recipe text into structured dictionary\"\"\"\n        recipe_dict = {\"title\": \"\", \"ingredients\": [], \"directions\": [], \"allergen_warnings\": []}\n        \n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            \n            if section.startswith(\"title:\"):\n                recipe_dict[\"title\"] = section.replace(\"title:\", \"\").strip().capitalize()\n            \n            elif section.startswith(\"ingredients:\"):\n                ingr_text = section.replace(\"ingredients:\", \"\").strip()\n                recipe_dict[\"ingredients\"] = [\n                    item.strip().capitalize() for item in ingr_text.split(\"--\") if item.strip()\n                ]\n            \n            elif section.startswith(\"directions:\"):\n                dir_text = section.replace(\"directions:\", \"\").strip()\n                recipe_dict[\"directions\"] = [\n                    item.strip().capitalize() for item in dir_text.split(\"--\") if item.strip()\n                ]\n        \n        return recipe_dict\n    \n    def check_allergens(self, recipe_dict):\n        \"\"\"Check for potentially problematic ingredient combinations\"\"\"\n        warnings = []\n        ingredients_lower = [ing.lower() for ing in recipe_dict[\"ingredients\"]]\n        \n        for ing1, ing2, reason in self.allergen_combinations:\n            # Check if both ingredients are present\n            has_ing1 = any(ing1 in ingredient for ingredient in ingredients_lower)\n            has_ing2 = any(ing2 in ingredient for ingredient in ingredients_lower)\n            \n            if has_ing1 and has_ing2:\n                warnings.append({\n                    \"ingredients\": (ing1, ing2),\n                    \"reason\": reason\n                })\n        \n        return warnings\n    \n    def format_for_display(self, recipe_dict):\n        \"\"\"Format recipe dictionary for display\"\"\"\n        display_text = f\"[TITLE]: {recipe_dict['title']}\\n\\n\"\n        \n        display_text += \"[INGREDIENTS]:\\n\"\n        for i, ingredient in enumerate(recipe_dict['ingredients']):\n            display_text += f\"  - {i+1}: {ingredient}\\n\"\n        \n        display_text += \"\\n[DIRECTIONS]:\\n\"\n        for i, step in enumerate(recipe_dict['directions']):\n            display_text += f\"  - {i+1}: {step}\\n\"\n        \n        # Add allergen warnings if present\n        if recipe_dict.get(\"allergen_warnings\", []):\n            display_text += \"\\n[⚠️ ALLERGEN WARNINGS]:\\n\"\n            for i, warning in enumerate(recipe_dict[\"allergen_warnings\"]):\n                display_text += f\"  - Warning {i+1}: {warning['ingredients'][0]} and {warning['ingredients'][1]} - {warning['reason']}\\n\"\n        \n        return display_text\n    \n    def evaluate_recipe_quality(self, recipe_dict):\n        \"\"\"Evaluate recipe quality based on simple heuristics\"\"\"\n        score = 0\n        max_score = 100\n        \n        # Check title\n        if recipe_dict[\"title\"] and len(recipe_dict[\"title\"]) > 3:\n            score += 10\n        \n        # Check ingredients\n        if recipe_dict[\"ingredients\"]:\n            score += min(len(recipe_dict[\"ingredients\"]) * 5, 30)  # Up to 30 points for ingredients\n            \n            # Check for ingredient variety (rough estimate)\n            unique_words = set()\n            for ingredient in recipe_dict[\"ingredients\"]:\n                unique_words.update(ingredient.lower().split())\n            score += min(len(unique_words) * 2, 20)  # Up to 20 points for variety\n        \n        # Check directions\n        if recipe_dict[\"directions\"]:\n            score += min(len(recipe_dict[\"directions\"]) * 5, 30)  # Up to 30 points for directions\n            \n            # Check for detailed instructions (rough estimate by length)\n            total_length = sum(len(step) for step in recipe_dict[\"directions\"])\n            score += min(total_length // 50, 10)  # Up to 10 points for detail\n        \n        # Deduct points for allergen warnings\n        allergen_warnings = recipe_dict.get(\"allergen_warnings\", [])\n        score -= len(allergen_warnings) * 15  # Deduct 15 points per allergen warning\n        \n        # Normalize to 100 (but don't go below 0)\n        normalized_score = max(min(score, max_score), 0)\n        \n        return normalized_score\n\n# Run the test\nif __name__ == \"__main__\":\n    tester = AllergenDetectionTester()\n    results = tester.test_allergen_combinations()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:30.460559Z","iopub.execute_input":"2025-04-10T10:33:30.460924Z","iopub.status.idle":"2025-04-10T10:33:35.181876Z","shell.execute_reply.started":"2025-04-10T10:33:30.460893Z","shell.execute_reply":"2025-04-10T10:33:35.181042Z"}},"outputs":[{"name":"stdout","text":"Loading model and tokenizer from ./recipe_model...\nModel not found, using t5-base for demonstration\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3b410636a54f54917a9c1dd700e20f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d436750eed76402dba738f40858605a9"}},"metadata":{}},{"name":"stdout","text":"Using GPU for inference\nTester initialized\n\n======= TESTING ALLERGEN COMBINATIONS =======\n\nTesting ingredients: fish, cheese, pasta, garlic, olive oil\nUsing mock recipe generation for demonstration\n\n[TITLE]: Fish recipe\n\n[INGREDIENTS]:\n  - 1: Fish\n  - 2: Cheese\n  - 3: Pasta\n  - 4: Garlic\n  - 5: Olive oil\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: fish and cheese - Fish and cheese can trigger food sensitivities\n\nQuality Score: 47/100\n--------------------------------------------------\nTesting ingredients: shrimp, milk, pasta, garlic, butter\nUsing mock recipe generation for demonstration\n\n[TITLE]: Shrimp recipe\n\n[INGREDIENTS]:\n  - 1: Shrimp\n  - 2: Milk\n  - 3: Pasta\n  - 4: Garlic\n  - 5: Butter\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: shrimp and milk - Shrimp and milk can trigger allergic reactions\n\nQuality Score: 45/100\n--------------------------------------------------\nTesting ingredients: tuna, yogurt, onions, celery, mayonnaise\nUsing mock recipe generation for demonstration\n\n[TITLE]: Tuna recipe\n\n[INGREDIENTS]:\n  - 1: Tuna\n  - 2: Yogurt\n  - 3: Onions\n  - 4: Celery\n  - 5: Mayonnaise\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\nQuality Score: 60/100\n--------------------------------------------------\nTesting ingredients: salmon, curd, lemon, dill, rice\nUsing mock recipe generation for demonstration\n\n[TITLE]: Salmon recipe\n\n[INGREDIENTS]:\n  - 1: Salmon\n  - 2: Curd\n  - 3: Lemon\n  - 4: Dill\n  - 5: Rice\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\nQuality Score: 60/100\n--------------------------------------------------\nTesting ingredients: chicken, rice, vegetables, soy sauce\nUsing mock recipe generation for demonstration\n\n[TITLE]: Chicken recipe\n\n[INGREDIENTS]:\n  - 1: Chicken\n  - 2: Rice\n  - 3: Vegetables\n  - 4: Soy sauce\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\nQuality Score: 55/100\n--------------------------------------------------\nTesting ingredients: shellfish, mango, coconut, lime, ginger\nUsing mock recipe generation for demonstration\n\n[TITLE]: Shellfish recipe\n\n[INGREDIENTS]:\n  - 1: Shellfish\n  - 2: Mango\n  - 3: Coconut\n  - 4: Lime\n  - 5: Ginger\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: shellfish and mango - Shellfish and mango can cause histamine reactions\n\nQuality Score: 45/100\n--------------------------------------------------\nTesting ingredients: peanuts, wheat flour, sugar, eggs, butter\nUsing mock recipe generation for demonstration\n\n[TITLE]: Peanuts recipe\n\n[INGREDIENTS]:\n  - 1: Peanuts\n  - 2: Wheat flour\n  - 3: Sugar\n  - 4: Eggs\n  - 5: Butter\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: peanuts and wheat - Peanuts and wheat can cause severe reactions in some people\n\nQuality Score: 47/100\n--------------------------------------------------\nTesting ingredients: strawberry, chocolate, cream, sugar, vanilla\nUsing mock recipe generation for demonstration\n\n[TITLE]: Strawberry recipe\n\n[INGREDIENTS]:\n  - 1: Strawberry\n  - 2: Chocolate\n  - 3: Cream\n  - 4: Sugar\n  - 5: Vanilla\n\n[DIRECTIONS]:\n  - 1: Mix ingredients\n  - 2: Cook\n  - 3: Serve\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: strawberry and chocolate - Strawberry and chocolate may trigger migraine in sensitive individuals\n\nQuality Score: 45/100\n--------------------------------------------------\n\n======= ALLERGEN DETECTION SUMMARY =======\n\nIngredients: fish, cheese, pasta, garlic, olive oil\nAllergen Warnings: 1\n  - fish + cheese: Fish and cheese can trigger food sensitivities\n\nIngredients: shrimp, milk, pasta, garlic, butter\nAllergen Warnings: 1\n  - shrimp + milk: Shrimp and milk can trigger allergic reactions\n\nIngredients: tuna, yogurt, onions, celery, mayonnaise\nAllergen Warnings: 0\n\nIngredients: salmon, curd, lemon, dill, rice\nAllergen Warnings: 0\n\nIngredients: chicken, rice, vegetables, soy sauce\nAllergen Warnings: 0\n\nIngredients: shellfish, mango, coconut, lime, ginger\nAllergen Warnings: 1\n  - shellfish + mango: Shellfish and mango can cause histamine reactions\n\nIngredients: peanuts, wheat flour, sugar, eggs, butter\nAllergen Warnings: 1\n  - peanuts + wheat: Peanuts and wheat can cause severe reactions in some people\n\nIngredients: strawberry, chocolate, cream, sugar, vanilla\nAllergen Warnings: 1\n  - strawberry + chocolate: Strawberry and chocolate may trigger migraine in sensitive individuals\n\nTotal Recipes: 8\nSafe Recipes: 3\nUnsafe Recipes: 5\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    EarlyStoppingCallback,\n    DataCollatorForSeq2Seq\n)\nfrom datasets import load_from_disk, DatasetDict # Import DatasetDict for type hint clarity\nimport time\nimport logging # Import logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n\ndef fine_tune_recipe_model(\n    base_model: str = \"t5-base\",\n    output_dir: str = \"./recipe_model_finetuned\", # Default to safer output dir\n    preprocessed_dataset_dir: str = \"preprocessed_recipe_dataset\",\n    num_train_epochs: int = 1,\n    learning_rate: float = 5e-5,\n    weight_decay: float = 0.01,\n    warmup_ratio: float = 0.1,\n    fp16: bool = True,\n    batch_size: int = 4\n):\n    \"\"\"\n    Fine-tune a T5 model for recipe generation using a preprocessed dataset.\n    Includes fix for compute_metrics error.\n    \"\"\"\n    logging.info(f\"Starting fine-tuning process for {base_model}...\")\n    logging.info(f\"--- Configuration ---\")\n    logging.info(f\"Output Directory: {output_dir}\")\n    logging.info(f\"Dataset Directory: {preprocessed_dataset_dir}\")\n    logging.info(f\"Epochs: {num_train_epochs}\")\n    logging.info(f\"Learning Rate: {learning_rate}\")\n    logging.info(f\"Batch Size: {batch_size}\")\n    logging.info(f\"Eval/Save Steps: 500\") # Hardcoded based on previous request\n    logging.info(f\"FP16 Enabled: {fp16}\")\n    logging.info(f\"---------------------\")\n\n    # --- 1. Load Preprocessed Dataset ---\n    try:\n        # Ensure the path exists\n        if not os.path.isdir(preprocessed_dataset_dir):\n             raise FileNotFoundError(f\"Dataset directory not found: {preprocessed_dataset_dir}\")\n\n        processed_dataset = load_from_disk(preprocessed_dataset_dir)\n        logging.info(f\"Loaded preprocessed dataset: {processed_dataset}\")\n\n        # Validate dataset structure\n        if not isinstance(processed_dataset, DatasetDict):\n            raise TypeError(f\"Expected loaded object to be a DatasetDict, but got {type(processed_dataset)}\")\n        if \"train\" not in processed_dataset:\n             raise ValueError(\"Dataset missing 'train' split.\")\n        if \"validation\" not in processed_dataset:\n             raise ValueError(\"Dataset missing 'validation' split.\")\n             # Test split is optional but recommended\n\n        logging.info(f\"Training examples: {len(processed_dataset['train'])}\")\n        logging.info(f\"Validation examples: {len(processed_dataset['validation'])}\")\n        if \"test\" in processed_dataset:\n            logging.info(f\"Test examples: {len(processed_dataset['test'])}\")\n\n        # Example calculation for steps\n        steps_per_epoch = len(processed_dataset['train']) // batch_size\n        total_steps = steps_per_epoch * num_train_epochs\n        logging.info(f\"Estimated steps per epoch: {steps_per_epoch}\")\n        logging.info(f\"Estimated total training steps: {total_steps}\")\n\n        # Verify necessary columns exist in a sample (important!)\n        sample = processed_dataset[\"train\"][0]\n        required_columns = ['input_ids', 'attention_mask', 'labels']\n        missing_columns = [col for col in required_columns if col not in sample]\n        if missing_columns:\n            raise ValueError(f\"Dataset samples missing required columns: {missing_columns}. Found keys: {list(sample.keys())}\")\n        logging.info(\"Dataset structure validated successfully.\")\n\n\n    except Exception as e:\n        logging.error(f\"Error loading or validating preprocessed dataset from '{preprocessed_dataset_dir}': {e}\", exc_info=True)\n        return None, None\n\n    # --- 2. Load Tokenizer and Model ---\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(base_model)\n        logging.info(f\"Loaded tokenizer: {base_model}\")\n\n        # Add special tokens\n        special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n        num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n        if num_added_tokens > 0:\n            logging.info(f\"Added {num_added_tokens} special tokens: {special_tokens['additional_special_tokens']}\")\n\n        model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n        logging.info(f\"Loaded base model: {base_model}\")\n\n        # Resize embeddings for new tokens\n        model.resize_token_embeddings(len(tokenizer))\n        logging.info(f\"Resized model token embeddings to size {len(tokenizer)}\")\n\n        # Check trainable parameters\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        if trainable_params == 0:\n            logging.warning(\"Model has no trainable parameters!\")\n        else:\n            logging.info(f\"Model trainable parameters: {trainable_params:,}\")\n\n    except Exception as e:\n        logging.error(f\"Error loading model or tokenizer '{base_model}': {e}\", exc_info=True)\n        return None, None\n\n    # --- 3. Training Arguments ---\n    run_name = f\"recipe-{base_model.split('/')[-1]}-{int(time.time())}\" # More specific run name\n    effective_eval_save_steps = 500 # Using the previously requested value\n\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,\n        # Strategies\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_strategy=\"steps\",\n        # Steps\n        eval_steps=effective_eval_save_steps,\n        save_steps=effective_eval_save_steps,\n        logging_steps=50, # Log loss more frequently\n        # Hyperparameters\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2, # Can often use larger batch size for eval\n        weight_decay=weight_decay,\n        num_train_epochs=num_train_epochs,\n        warmup_ratio=warmup_ratio,\n        # Checkpointing and Best Model\n        save_total_limit=3,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"loss\", # Use validation loss to find the best model\n        greater_is_better=False,\n        # Performance\n        fp16=fp16, # Use mixed precision if available/enabled\n        optim=\"adamw_torch\", # Recommended optimizer\n        dataloader_num_workers=os.cpu_count() // 2 if os.cpu_count() else 2, # Adjust based on your system\n        # Generation settings (for predict_with_generate)\n        predict_with_generate=True,\n        generation_max_length=512,\n        generation_num_beams=4,\n        # Reporting\n        report_to=[\"tensorboard\"],\n        run_name=run_name,\n    )\n\n    # --- 4. Data Collator ---\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=\"max_length\", # Ensure all sequences in a batch have the same length\n        max_length=512, # Max length for tokenization\n        label_pad_token_id=tokenizer.pad_token_id # Ensure labels are padded correctly\n    )\n\n    # --- 5. Compute Metrics (CORRECTED) ---\n    def compute_metrics(eval_preds):\n        # The Trainer calculates and logs validation loss automatically.\n        # Since metric_for_best_model='loss', this loss is used for\n        # checkpointing the best model and for early stopping.\n        # We don't need to calculate any additional metrics here,\n        # so we return an empty dictionary.\n        # If you wanted ROUGE/BLEU, you would decode preds/labels and compute them here.\n        # predictions, labels = eval_preds\n        # decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        # Add metric calculation here... e.g., rouge = compute_rouge(decoded_preds, decoded_labels)\n        return {} # Return empty dict as loss is handled internally\n\n    # --- 6. Initialize Trainer ---\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_dataset[\"train\"],\n        eval_dataset=processed_dataset[\"validation\"],\n        data_collator=data_collator,\n        tokenizer=tokenizer, # Pass tokenizer for saving correctly\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)] # Stop if loss doesn't improve significantly\n    )\n\n    # --- 7. Train ---\n    logging.info(\"Starting fine-tuning...\")\n    train_result = None\n    try:\n        train_result = trainer.train()\n        logging.info(\"Training completed successfully!\")\n\n        # Log training metrics\n        metrics = train_result.metrics\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n\n    except Exception as e:\n        logging.error(f\"An error occurred during training: {e}\", exc_info=True)\n        # The debug print from before is less useful now with logging/validation\n        return None, None # Return None if training failed\n\n    # --- 8. Save Final Model ---\n    # save_model saves the tokenizer too if passed to Trainer constructor\n    try:\n        logging.info(f\"Saving the best model to {output_dir}...\")\n        trainer.save_model(output_dir)\n        # tokenizer.save_pretrained(output_dir) # Trainer should handle this\n        logging.info(f\"Best model saved successfully.\")\n    except Exception as e:\n        logging.error(f\"Could not save the final model: {e}\", exc_info=True)\n\n\n    # --- 9. Evaluate on Test Set ---\n    if \"test\" in processed_dataset:\n        logging.info(\"Evaluating the best model on the test set...\")\n        try:\n            test_results = trainer.evaluate(eval_dataset=processed_dataset[\"test\"])\n            logging.info(f\"Test set evaluation results: {test_results}\")\n            trainer.log_metrics(\"test\", test_results)\n            trainer.save_metrics(\"test\", test_results)\n        except Exception as e:\n            logging.error(f\"Could not evaluate on the test set: {e}\", exc_info=True)\n    else:\n        logging.warning(\"No 'test' split found in the dataset. Skipping final test evaluation.\")\n\n    logging.info(\"Fine-tuning script finished.\")\n    # Return the trained model and tokenizer in memory (may use significant RAM)\n    # It's often better to load the saved model later using AutoModel...from_pretrained(output_dir)\n    return model, tokenizer\n\n# --- Main Execution Block ---\nif __name__ == \"__main__\":\n    print(\"============================================\")\n    print(\" Recipe Model Fine-Tuning Script \")\n    print(\"============================================\")\n    print(\"Ensure 'accelerate' library is installed for optimized training and FP16: pip install accelerate -U\")\n    print(\"Ensure your preprocessed dataset exists at './preprocessed_recipe_dataset/'\")\n    print(\"--------------------------------------------\")\n\n    # Determine FP16 availability\n    use_fp16 = torch.cuda.is_available()\n    if use_fp16:\n        print(\"CUDA detected. FP16 training will be enabled.\")\n    else:\n        print(\"CUDA not detected. FP16 training will be disabled (runs on CPU or MPS).\")\n\n    # Define dataset and output paths\n    dataset_path = \"preprocessed_recipe_dataset\"\n    model_output_path = \"./recipe_model_finetuned\" # Separate output dir\n\n    # --- Call the Fine-Tuning Function ---\n    model, tokenizer = fine_tune_recipe_model(\n         base_model=\"t5-base\",\n         preprocessed_dataset_dir=dataset_path,\n         output_dir=model_output_path,\n         num_train_epochs=1,\n         batch_size=4, # Keep batch size small if memory is limited\n         fp16=use_fp16\n     )\n\n    # --- Report Outcome ---\n    print(\"--------------------------------------------\")\n    if model and tokenizer:\n        print(f\"✅ Fine-tuning process finished successfully.\")\n        print(f\"   Model and tokenizer returned (in memory).\")\n        print(f\"   Best model saved to: {model_output_path}\")\n    else:\n        print(f\"❌ Fine-tuning process did not complete successfully.\")\n        print(f\"   Please check the log messages above for errors.\")\n    print(\"============================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:48.036088Z","iopub.execute_input":"2025-04-10T10:33:48.036404Z","iopub.status.idle":"2025-04-10T11:45:10.506527Z","shell.execute_reply.started":"2025-04-10T10:33:48.036377Z","shell.execute_reply":"2025-04-10T11:45:10.505692Z"}},"outputs":[{"name":"stdout","text":"============================================\n Recipe Model Fine-Tuning Script \n============================================\nEnsure 'accelerate' library is installed for optimized training and FP16: pip install accelerate -U\nEnsure your preprocessed dataset exists at './preprocessed_recipe_dataset/'\n--------------------------------------------\nCUDA detected. FP16 training will be enabled.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-eef40f8c2d82>:180: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 1:01:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.495000</td>\n      <td>0.410472</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.364000</td>\n      <td>0.290418</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.316100</td>\n      <td>0.256346</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.296600</td>\n      <td>0.248394</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =  4537089GF\n  train_loss               =     0.8938\n  train_runtime            = 1:01:52.17\n  train_samples_per_second =      2.155\n  train_steps_per_second   =      0.539\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 09:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** test metrics *****\n  epoch                   =        1.0\n  eval_loss               =     0.2514\n  eval_runtime            = 0:09:25.73\n  eval_samples_per_second =      1.768\n  eval_steps_per_second   =      0.221\n--------------------------------------------\n✅ Fine-tuning process finished successfully.\n   Model and tokenizer returned (in memory).\n   Best model saved to: ./recipe_model_finetuned\n============================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# First, make sure your Google Drive is mounted\nfrom kaggle_web_client import KaggleWebClient\nfrom kaggle_datasets import KaggleDatasets\nimport os\nimport zipfile\nimport shutil\n\n# Path to your fine-tuned model in Kaggle\nmodel_path = \"./recipe_model_finetuned\"  # Update this to your model path\n\n# Create a zip file of your model\nzip_path = \"./recipe_model_finetuned.zip\"\nprint(f\"Creating zip file at {zip_path}...\")\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(model_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, start=model_path)\n            print(f\"Adding {arcname} to zip...\")\n            zipf.write(file_path, arcname=arcname)\n\nprint(f\"Zip file created successfully at {zip_path}\")\n\n# Path in Google Drive where you want to save\ndrive_path = \"/kaggle/drive/MyDrive/RecipeGenie\"\n\n# Create the directory if it doesn't exist\nos.makedirs(drive_path, exist_ok=True)\n\n# Copy the zip file to Google Drive\nshutil.copy(zip_path, os.path.join(drive_path, \"recipe_model_finetuned.zip\"))\n\nprint(f\"Model zip file has been saved to Google Drive at {drive_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T11:50:24.180622Z","iopub.execute_input":"2025-04-10T11:50:24.181008Z","iopub.status.idle":"2025-04-10T11:58:04.213346Z","shell.execute_reply.started":"2025-04-10T11:50:24.180972Z","shell.execute_reply":"2025-04-10T11:58:04.210213Z"}},"outputs":[{"name":"stdout","text":"Creating zip file at ./recipe_model_finetuned.zip...\nAdding generation_config.json to zip...\nAdding training_args.bin to zip...\nAdding train_results.json to zip...\nAdding special_tokens_map.json to zip...\nAdding config.json to zip...\nAdding tokenizer.json to zip...\nAdding spiece.model to zip...\nAdding model.safetensors to zip...\nAdding all_results.json to zip...\nAdding test_results.json to zip...\nAdding added_tokens.json to zip...\nAdding tokenizer_config.json to zip...\nAdding runs/Apr10_10-33-49_ae2120e9f195/events.out.tfevents.1744285510.ae2120e9f195.31.1 to zip...\nAdding runs/Apr10_10-33-49_ae2120e9f195/events.out.tfevents.1744281230.ae2120e9f195.31.0 to zip...\nAdding checkpoint-1500/generation_config.json to zip...\nAdding checkpoint-1500/training_args.bin to zip...\nAdding checkpoint-1500/scheduler.pt to zip...\nAdding checkpoint-1500/special_tokens_map.json to zip...\nAdding checkpoint-1500/config.json to zip...\nAdding checkpoint-1500/tokenizer.json to zip...\nAdding checkpoint-1500/spiece.model to zip...\nAdding checkpoint-1500/rng_state.pth to zip...\nAdding checkpoint-1500/model.safetensors to zip...\nAdding checkpoint-1500/trainer_state.json to zip...\nAdding checkpoint-1500/optimizer.pt to zip...\nAdding checkpoint-1500/added_tokens.json to zip...\nAdding checkpoint-1500/tokenizer_config.json to zip...\nAdding checkpoint-1000/generation_config.json to zip...\nAdding checkpoint-1000/training_args.bin to zip...\nAdding checkpoint-1000/scheduler.pt to zip...\nAdding checkpoint-1000/special_tokens_map.json to zip...\nAdding checkpoint-1000/config.json to zip...\nAdding checkpoint-1000/tokenizer.json to zip...\nAdding checkpoint-1000/spiece.model to zip...\nAdding checkpoint-1000/rng_state.pth to zip...\nAdding checkpoint-1000/model.safetensors to zip...\nAdding checkpoint-1000/trainer_state.json to zip...\nAdding checkpoint-1000/optimizer.pt to zip...\nAdding checkpoint-1000/added_tokens.json to zip...\nAdding checkpoint-1000/tokenizer_config.json to zip...\nAdding checkpoint-2000/generation_config.json to zip...\nAdding checkpoint-2000/training_args.bin to zip...\nAdding checkpoint-2000/scheduler.pt to zip...\nAdding checkpoint-2000/special_tokens_map.json to zip...\nAdding checkpoint-2000/config.json to zip...\nAdding checkpoint-2000/tokenizer.json to zip...\nAdding checkpoint-2000/spiece.model to zip...\nAdding checkpoint-2000/rng_state.pth to zip...\nAdding checkpoint-2000/model.safetensors to zip...\nAdding checkpoint-2000/trainer_state.json to zip...\nAdding checkpoint-2000/optimizer.pt to zip...\nAdding checkpoint-2000/added_tokens.json to zip...\nAdding checkpoint-2000/tokenizer_config.json to zip...\nZip file created successfully at ./recipe_model_finetuned.zip\nModel zip file has been saved to Google Drive at /kaggle/drive/MyDrive/RecipeGenie\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from IPython.display import FileLink, display\n\n# Zip your model\nimport zipfile\nimport os\n\nmodel_path = \"./recipe_model_finetuned\"\nzip_path = \"./recipe_model_finetuned.zip\"\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(model_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, start=model_path)\n            zipf.write(file_path, arcname=arcname)\n\nprint(f\"Created zip file at {zip_path}\")\n\n# Generate download link\nprint(\"Click the link below to download the model zip file:\")\ndisplay(FileLink(zip_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T12:01:07.095691Z","iopub.execute_input":"2025-04-10T12:01:07.095999Z","iopub.status.idle":"2025-04-10T12:08:32.716243Z","shell.execute_reply.started":"2025-04-10T12:01:07.095977Z","shell.execute_reply":"2025-04-10T12:08:32.715518Z"}},"outputs":[{"name":"stdout","text":"Created zip file at ./recipe_model_finetuned.zip\nClick the link below to download the model zip file:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/recipe_model_finetuned.zip","text/html":"<a href='./recipe_model_finetuned.zip' target='_blank'>./recipe_model_finetuned.zip</a><br>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport time\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nclass RecipePostProcessor:\n    def __init__(self, tokenizer):\n        \"\"\"\n        Initialize post-processor with tokenizer for handling special tokens.\n        Includes allergen detection and additional recipe processing.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.special_tokens = tokenizer.all_special_tokens\n        self.tokens_map = {\n            \"<sep>\": \"--\",\n            \"<section>\": \"\\n\"\n        }\n        \n        # Allergen combinations to avoid\n        self.allergen_combinations = [\n            # Format: (ingredient1, ingredient2, reason)\n            (\"fish\", \"dairy\", \"Fish and dairy combinations can cause digestive issues for many people\"),\n            (\"fish\", \"yogurt\", \"Fish and yogurt may cause digestive issues\"),\n            (\"fish\", \"milk\", \"Fish and milk can cause adverse reactions in some individuals\"),\n            (\"fish\", \"curd\", \"Fish and curd combinations may cause allergic reactions\"),\n            (\"fish\", \"cheese\", \"Fish and cheese can trigger food sensitivities\"),\n            (\"tuna\", \"cheese\", \"Tuna and cheese can trigger food sensitivities\"),\n            (\"salmon\", \"cheese\", \"Salmon and cheese can trigger food sensitivities\"),\n            (\"shellfish\", \"dairy\", \"Shellfish and dairy combinations can trigger allergic reactions\"),\n            (\"shellfish\", \"milk\", \"Shellfish and milk can trigger allergic reactions\"),\n            (\"shrimp\", \"milk\", \"Shrimp and milk can trigger allergic reactions\"),\n            (\"peanuts\", \"gluten\", \"Peanuts and gluten can cause severe reactions in some people\"),\n            (\"peanuts\", \"wheat\", \"Peanuts and wheat can cause severe reactions in some people\"),\n            (\"shellfish\", \"mango\", \"Shellfish and mango can cause histamine reactions\"),\n            (\"strawberry\", \"chocolate\", \"Strawberry and chocolate may trigger migraine in sensitive individuals\")\n        ]\n    \n    def postprocess_text(self, text):\n        \"\"\"\n        Post-process generated recipe text:\n        1. Remove special tokens except our custom ones\n        2. Replace mapped tokens with their human-readable versions\n        3. Format into structured recipe\n        4. Check for allergen combinations\n        \"\"\"\n        # Remove special tokens except our custom ones\n        for token in self.special_tokens:\n            if token not in self.tokens_map:\n                text = text.replace(token, \"\")\n        \n        # Replace mapped tokens with readable versions\n        for k, v in self.tokens_map.items():\n            text = text.replace(k, v)\n        \n        # Format structured recipe\n        recipe_dict = self._format_recipe(text)\n        \n        # Check for allergen combinations\n        allergen_warnings = self.check_allergens(recipe_dict)\n        recipe_dict[\"allergen_warnings\"] = allergen_warnings\n        \n        return recipe_dict\n    \n    def _format_recipe(self, text):\n        \"\"\"Format recipe text into structured dictionary\"\"\"\n        recipe_dict = {\"title\": \"\", \"ingredients\": [], \"directions\": [], \"allergen_warnings\": []}\n        \n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            \n            if section.startswith(\"title:\"):\n                recipe_dict[\"title\"] = section.replace(\"title:\", \"\").strip().capitalize()\n            \n            elif section.startswith(\"ingredients:\"):\n                ingr_text = section.replace(\"ingredients:\", \"\").strip()\n                recipe_dict[\"ingredients\"] = [\n                    item.strip().capitalize() for item in ingr_text.split(\"--\") if item.strip()\n                ]\n            \n            elif section.startswith(\"directions:\"):\n                dir_text = section.replace(\"directions:\", \"\").strip()\n                recipe_dict[\"directions\"] = [\n                    item.strip().capitalize() for item in dir_text.split(\"--\") if item.strip()\n                ]\n        \n        return recipe_dict\n    \n    def check_allergens(self, recipe_dict):\n        \"\"\"Check for potentially problematic ingredient combinations\"\"\"\n        warnings = []\n        ingredients_lower = [ing.lower() for ing in recipe_dict[\"ingredients\"]]\n        \n        for ing1, ing2, reason in self.allergen_combinations:\n            # Check if both ingredients are present\n            has_ing1 = any(ing1 in ingredient for ingredient in ingredients_lower)\n            has_ing2 = any(ing2 in ingredient for ingredient in ingredients_lower)\n            \n            if has_ing1 and has_ing2:\n                warnings.append({\n                    \"ingredients\": (ing1, ing2),\n                    \"reason\": reason\n                })\n        \n        return warnings\n    \n    def format_for_display(self, recipe_dict):\n        \"\"\"Format recipe dictionary for display\"\"\"\n        display_text = f\"[TITLE]: {recipe_dict['title']}\\n\\n\"\n        \n        display_text += \"[INGREDIENTS]:\\n\"\n        for i, ingredient in enumerate(recipe_dict['ingredients']):\n            display_text += f\"  - {i+1}: {ingredient}\\n\"\n        \n        display_text += \"\\n[DIRECTIONS]:\\n\"\n        for i, step in enumerate(recipe_dict['directions']):\n            display_text += f\"  - {i+1}: {step}\\n\"\n        \n        # Add allergen warnings if present\n        if recipe_dict.get(\"allergen_warnings\", []):\n            display_text += \"\\n[⚠️ ALLERGEN WARNINGS]:\\n\"\n            for i, warning in enumerate(recipe_dict[\"allergen_warnings\"]):\n                display_text += f\"  - Warning {i+1}: {warning['ingredients'][0]} and {warning['ingredients'][1]} - {warning['reason']}\\n\"\n        \n        return display_text\n    \n    def evaluate_recipe_quality(self, recipe_dict):\n        \"\"\"Evaluate recipe quality based on simple heuristics\"\"\"\n        score = 0\n        max_score = 100\n        \n        # Check title\n        if recipe_dict[\"title\"] and len(recipe_dict[\"title\"]) > 3:\n            score += 10\n        \n        # Check ingredients\n        if recipe_dict[\"ingredients\"]:\n            score += min(len(recipe_dict[\"ingredients\"]) * 5, 30)  # Up to 30 points for ingredients\n            \n            # Check for ingredient variety (rough estimate)\n            unique_words = set()\n            for ingredient in recipe_dict[\"ingredients\"]:\n                unique_words.update(ingredient.lower().split())\n            score += min(len(unique_words) * 2, 20)  # Up to 20 points for variety\n        \n        # Check directions\n        if recipe_dict[\"directions\"]:\n            score += min(len(recipe_dict[\"directions\"]) * 5, 30)  # Up to 30 points for directions\n            \n            # Check for detailed instructions (rough estimate by length)\n            total_length = sum(len(step) for step in recipe_dict[\"directions\"])\n            score += min(total_length // 50, 10)  # Up to 10 points for detail\n        \n        # Deduct points for allergen warnings\n        allergen_warnings = recipe_dict.get(\"allergen_warnings\", [])\n        score -= len(allergen_warnings) * 15  # Deduct 15 points per allergen warning\n        \n        # Normalize to 100 (but don't go below 0)\n        normalized_score = max(min(score, max_score), 0)\n        \n        return normalized_score\n\n\nclass RecipeModelTester:\n    def __init__(self, model_path=\"./recipe_model_finetuned\"):\n        \"\"\"Initialize the model tester with the path to the fine-tuned model.\"\"\"\n        print(f\"Loading model and tokenizer from {model_path}...\")\n        \n        # Try to load the fine-tuned model, or fall back to the pre-trained model\n        try:\n            if os.path.exists(model_path):\n                self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n                self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n                print(f\"Successfully loaded fine-tuned model from {model_path}\")\n            else:\n                # Fall back to original pre-trained model\n                print(f\"Fine-tuned model not found. Falling back to pre-trained model.\")\n                self.tokenizer = AutoTokenizer.from_pretrained(\"flax-community/t5-recipe-generation\")\n                self.model = AutoModelForSeq2SeqLM.from_pretrained(\"flax-community/t5-recipe-generation\")\n                print(\"Successfully loaded pre-trained 'flax-community/t5-recipe-generation' model\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Falling back to t5-base model as a last resort\")\n            self.tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n            self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n            print(\"Loaded t5-base model\")\n        \n        # Add special tokens if needed\n        if \"<sep>\" not in self.tokenizer.get_vocab():\n            special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n            self.tokenizer.add_special_tokens(special_tokens)\n            self.model.resize_token_embeddings(len(self.tokenizer))\n            print(\"Added special tokens to tokenizer\")\n        \n        # Optimize model for inference\n        self.model = self.model.eval()\n        if torch.cuda.is_available():\n            print(\"Using GPU for inference\")\n            self.model = self.model.cuda()\n            try:\n                self.model = self.model.half()  # Use FP16 for faster inference\n                print(\"Using half precision for faster inference\")\n            except:\n                print(\"Half precision not supported, using full precision instead\")\n        else:\n            print(\"Using CPU for inference\")\n        \n        # Default generation parameters\n        self.generation_kwargs = {\n            \"max_length\": 512,\n            \"min_length\": 64,\n            \"no_repeat_ngram_size\": 3,\n            \"num_beams\": 4,\n            \"early_stopping\": True,\n            \"length_penalty\": 1.2,\n            \"do_sample\": True,\n            \"temperature\": 0.8,\n            \"top_k\": 50,\n            \"top_p\": 0.95\n        }\n        \n        # Initialize post-processor\n        self.postprocessor = RecipePostProcessor(self.tokenizer)\n        \n        print(\"Model initialized and ready for testing\")\n\n    def generate_recipe(self, ingredients, **generation_params):\n        \"\"\"Generate a recipe from a list of ingredients with optional parameters.\"\"\"\n        # Format input\n        if isinstance(ingredients, list):\n            ingredients = \", \".join(ingredients)\n        \n        input_text = f\"items: {ingredients}\"\n        \n        # Tokenize\n        inputs = self.tokenizer(\n            input_text, \n            max_length=256, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        \n        # Move to GPU if available\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        \n        # Update generation parameters if provided\n        gen_kwargs = self.generation_kwargs.copy()\n        gen_kwargs.update(generation_params)\n        \n        # Generate\n        start_time = time.time()\n        with torch.no_grad():\n            output_ids = self.model.generate(\n                **inputs,\n                **gen_kwargs\n            )\n        generation_time = time.time() - start_time\n        \n        # Decode and post-process\n        generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=False)\n        recipe_dict = self.postprocessor.postprocess_text(generated_text)\n        formatted_text = self.postprocessor.format_for_display(recipe_dict)\n        \n        # Evaluate quality\n        quality_score = self.postprocessor.evaluate_recipe_quality(recipe_dict)\n        \n        return {\n            \"raw_text\": generated_text,\n            \"recipe_dict\": recipe_dict,\n            \"formatted_text\": formatted_text,\n            \"generation_time\": generation_time,\n            \"quality_score\": quality_score,\n            \"allergen_warnings\": recipe_dict.get(\"allergen_warnings\", [])\n        }\n    \n    def test_allergen_combinations(self):\n        \"\"\"Test various allergen combinations.\"\"\"\n        test_combinations = [\n            \"fish, cheese, pasta, garlic, olive oil\",\n            \"shrimp, milk, pasta, garlic, butter\",\n            \"tuna, yogurt, onions, celery, mayonnaise\",\n            \"salmon, curd, lemon, dill, rice\",\n            \"chicken, rice, vegetables, soy sauce\",  # Non-allergenic control\n            \"shellfish, mango, coconut, lime, ginger\",\n            \"peanuts, wheat flour, sugar, eggs, butter\",\n            \"strawberry, chocolate, cream, sugar, vanilla\"\n        ]\n        \n        results = []\n        \n        print(\"\\n======= TESTING ALLERGEN COMBINATIONS =======\\n\")\n        \n        for ingredients in test_combinations:\n            print(f\"Testing ingredients: {ingredients}\")\n            \n            # Generate a recipe\n            result = self.generate_recipe(ingredients)\n            results.append(result)\n            \n            # Print the result\n            print(f\"\\n{result['formatted_text']}\")\n            print(f\"Quality Score: {result['quality_score']}/100\")\n            print(\"-\" * 50)\n            \n        # Summary of allergen detections\n        print(\"\\n======= ALLERGEN DETECTION SUMMARY =======\\n\")\n        for i, result in enumerate(results):\n            warnings = result.get(\"allergen_warnings\", [])\n            print(f\"Recipe {i+1} - Ingredients: {test_combinations[i]}\")\n            print(f\"Allergen Warnings: {len(warnings)}\")\n            for warning in warnings:\n                print(f\"  - {warning['ingredients'][0]} + {warning['ingredients'][1]}: {warning['reason']}\")\n            print()\n            \n        # Count safe vs. unsafe recipes\n        safe_recipes = [r for r in results if not r.get(\"allergen_warnings\")]\n        print(f\"Total Recipes: {len(results)}\")\n        print(f\"Safe Recipes: {len(safe_recipes)}\")\n        print(f\"Unsafe Recipes: {len(results) - len(safe_recipes)}\")\n        \n        return results\n    \n    def run_benchmark_test(self, test_samples, batch_size=4):\n        \"\"\"Run a benchmark test on multiple ingredient lists.\"\"\"\n        print(f\"Running benchmark with {len(test_samples)} test samples...\")\n        results = []\n        quality_scores = []\n        generation_times = []\n        allergen_counts = []\n        \n        # Process test samples\n        for i in tqdm(range(0, len(test_samples), batch_size)):\n            batch = test_samples[i:i+batch_size]\n            batch_results = []\n            \n            for ingredients in batch:\n                result = self.generate_recipe(ingredients)\n                batch_results.append(result)\n                \n                quality_scores.append(result[\"quality_score\"])\n                generation_times.append(result[\"generation_time\"])\n                allergen_counts.append(len(result.get(\"allergen_warnings\", [])))\n            \n            results.extend(batch_results)\n        \n        # Calculate statistics\n        avg_quality = np.mean(quality_scores)\n        avg_time = np.mean(generation_times)\n        avg_allergens = np.mean(allergen_counts)\n        \n        print(f\"Benchmark complete: {len(results)} recipes generated\")\n        print(f\"Average generation time: {avg_time:.3f} seconds\")\n        print(f\"Average quality score: {avg_quality:.1f}/100\")\n        print(f\"Average allergen warnings per recipe: {avg_allergens:.2f}\")\n        \n        # Plot quality distribution\n        plt.figure(figsize=(10, 5))\n        plt.hist(quality_scores, bins=10, alpha=0.7)\n        plt.title('Recipe Quality Score Distribution')\n        plt.xlabel('Quality Score')\n        plt.ylabel('Count')\n        plt.savefig('quality_distribution.png')\n        print(f\"Quality distribution plot saved to 'quality_distribution.png'\")\n        \n        # Plot allergen warnings\n        plt.figure(figsize=(10, 5))\n        plt.hist(allergen_counts, bins=max(allergen_counts)+1, alpha=0.7)\n        plt.title('Allergen Warnings Distribution')\n        plt.xlabel('Number of Warnings')\n        plt.ylabel('Count')\n        plt.savefig('allergen_distribution.png')\n        print(f\"Allergen distribution plot saved to 'allergen_distribution.png'\")\n        \n        return results, quality_scores, generation_times, allergen_counts\n\nif __name__ == \"__main__\":\n    # Test the model\n    tester = RecipeModelTester()\n    \n    # Test with individual ingredients including potentially allergenic combinations\n    test_samples = [\n        \"chicken, rice, garlic, onion, bell pepper\",\n        \"beef, potatoes, carrots, onion, garlic\",\n        \"fish, cheese, lemon, pasta, herbs\",  # Potentially allergenic\n        \"shrimp, milk, garlic, butter, pasta\",  # Potentially allergenic\n        \"salmon, curd, dill, rice, lemon\",  # Potentially allergenic\n        \"tofu, vegetables, soy sauce, ginger, garlic\",\n        \"chocolate, strawberry, cream, vanilla, sugar\"  # Potentially allergenic\n    ]\n    \n    # Generate a single recipe\n    print(\"\\nGenerating a sample recipe:\")\n    result = tester.generate_recipe(test_samples[0])\n    print(result[\"formatted_text\"])\n    print(f\"Generation time: {result['generation_time']:.3f} seconds\")\n    print(f\"Quality score: {result['quality_score']}/100\\n\")\n    \n    # Run allergen tests\n    print(\"\\nTesting allergen detection:\")\n    tester.test_allergen_combinations()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T07:38:08.846204Z","iopub.execute_input":"2025-04-10T07:38:08.846532Z","iopub.status.idle":"2025-04-10T07:38:32.035785Z","shell.execute_reply.started":"2025-04-10T07:38:08.846507Z","shell.execute_reply":"2025-04-10T07:38:32.035006Z"}},"outputs":[{"name":"stdout","text":"Loading model and tokenizer from ./recipe_model_finetuned...\nSuccessfully loaded fine-tuned model from ./recipe_model_finetuned\nUsing GPU for inference\nUsing half precision for faster inference\nModel initialized and ready for testing\n\nGenerating a sample recipe:\n[TITLE]: Grilled onion stir-fry\n\n[INGREDIENTS]:\n  - 1: Chicken\n  - 2: Rice\n  - 3: Garlic\n  - 4: Onion\n  - 5: Bell pepper\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pan to 388 degrees\n  - 3: Mix chicken and rice together\n  - 4: Add garlic, onion and cook for 26 minutes\n  - 5: Stir in bell pepper\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\nGeneration time: 2.053 seconds\nQuality score: 80/100\n\n\nTesting allergen detection:\n\n======= TESTING ALLERGEN COMBINATIONS =======\n\nTesting ingredients: fish, cheese, pasta, garlic, olive oil\n\n[TITLE]: Grilled olive oil pasta\n\n[INGREDIENTS]:\n  - 1: Fish\n  - 2: Cheese\n  - 3: Pasta\n  - 4: Garlic\n  - 5: Olive oil\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 337 degrees\n  - 3: Mix fish and cheese together\n  - 4: Add pasta, garlic and cook for 18 minutes\n  - 5: Stir in olive oil\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: fish and cheese - Fish and cheese can trigger food sensitivities\n\nQuality Score: 65/100\n--------------------------------------------------\nTesting ingredients: shrimp, milk, pasta, garlic, butter\n\n[TITLE]: Grilled pasta casserole\n\n[INGREDIENTS]:\n  - 1: Shrimp\n  - 2: Milk\n  - 3: Pasta\n  - 4: Garlic\n  - 5: Butter\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 317 degrees\n  - 3: Mix shrimp and milk together\n  - 4: Add pasta, garlic and cook for 8 minutes\n  - 5: Stir in butter\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: shrimp and milk - Shrimp and milk can trigger allergic reactions\n\nQuality Score: 63/100\n--------------------------------------------------\nTesting ingredients: tuna, yogurt, onions, celery, mayonnaise\n\n[TITLE]: Sautéed tuna salad\n\n[INGREDIENTS]:\n  - 1: Tuna\n  - 2: Yogurt\n  - 3: Onions\n  - 4: Celery\n  - 5: Mayonnaise\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat skillet to 389 degrees\n  - 3: Mix tuna and yogurt together\n  - 4: Add onions, celery and cook for 15 minutes\n  - 5: Stir in mayonnaisa\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\nQuality Score: 78/100\n--------------------------------------------------\nTesting ingredients: salmon, curd, lemon, dill, rice\n\n[TITLE]: Sautéed lemon casserole\n\n[INGREDIENTS]:\n  - 1: Salmon\n  - 2: Curd\n  - 3: Lemon\n  - 4: Dill\n  - 5: Rice\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 358 degrees\n  - 3: Mix salmon and curd together\n  - 4: Add lemon, dll and cook for 7 minutes\n  - 5: Stir in rice\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\nQuality Score: 78/100\n--------------------------------------------------\nTesting ingredients: chicken, rice, vegetables, soy sauce\n\n[TITLE]: Roasted soy sauce stir-fry\n\n[INGREDIENTS]:\n  - 1: Chicken\n  - 2: Rice\n  - 3: Vegetables\n  - 4: Soy sauce\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 337 degrees\n  - 3: Mix chicken and rice together\n  - 4: Add vegetables, soy soup and cook for 14 minutes\n  - 5: Stir in\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\nQuality Score: 73/100\n--------------------------------------------------\nTesting ingredients: shellfish, mango, coconut, lime, ginger\n\n[TITLE]: Grilled coconut casserole\n\n[INGREDIENTS]:\n  - 1: Shellfish\n  - 2: Mango\n  - 3: Coconut\n  - 4: Lime\n  - 5: Ginger\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 388 degrees\n  - 3: Mix shellfish and mango together\n  - 4: Add coconut, lime and cook for 14 minutes\n  - 5: Stir in ginger\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: shellfish and mango - Shellfish and mango can cause histamine reactions\n\nQuality Score: 63/100\n--------------------------------------------------\nTesting ingredients: peanuts, wheat flour, sugar, eggs, butter\n\n[TITLE]: Sautéed wheat flour stir-fry\n\n[INGREDIENTS]:\n  - 1: Peanuts\n  - 2: Wheat flour\n  - 3: Sugar\n  - 4: Eggs\n  - 5: Butter\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 337 degrees\n  - 3: Mix peanuts and wheat flour together\n  - 4: Add sugar, eggs and cook for 13 minutes\n  - 5: Stir in butter\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: peanuts and wheat - Peanuts and wheat can cause severe reactions in some people\n\nQuality Score: 65/100\n--------------------------------------------------\nTesting ingredients: strawberry, chocolate, cream, sugar, vanilla\n\n[TITLE]: Roasted vanilla soup\n\n[INGREDIENTS]:\n  - 1: Strawberry\n  - 2: Chocolate\n  - 3: Cream\n  - 4: Sugar\n  - 5: Vanilla\n\n[DIRECTIONS]:\n  - 1: Prepare all the ingredients\n  - 2: Heat pot to 326 degrees\n  - 3: Mix strawberry and chocolate together\n  - 4: Add cream, sugar and cook for 26 minutes\n  - 5: Stir in vanilla\n  - 6: Season with salt and pepper to taste\n  - 7: Serve hot\n\n[⚠️ ALLERGEN WARNINGS]:\n  - Warning 1: strawberry and chocolate - Strawberry and chocolate may trigger migraine in sensitive individuals\n\nQuality Score: 63/100\n--------------------------------------------------\n\n======= ALLERGEN DETECTION SUMMARY =======\n\nRecipe 1 - Ingredients: fish, cheese, pasta, garlic, olive oil\nAllergen Warnings: 1\n  - fish + cheese: Fish and cheese can trigger food sensitivities\n\nRecipe 2 - Ingredients: shrimp, milk, pasta, garlic, butter\nAllergen Warnings: 1\n  - shrimp + milk: Shrimp and milk can trigger allergic reactions\n\nRecipe 3 - Ingredients: tuna, yogurt, onions, celery, mayonnaise\nAllergen Warnings: 0\n\nRecipe 4 - Ingredients: salmon, curd, lemon, dill, rice\nAllergen Warnings: 0\n\nRecipe 5 - Ingredients: chicken, rice, vegetables, soy sauce\nAllergen Warnings: 0\n\nRecipe 6 - Ingredients: shellfish, mango, coconut, lime, ginger\nAllergen Warnings: 1\n  - shellfish + mango: Shellfish and mango can cause histamine reactions\n\nRecipe 7 - Ingredients: peanuts, wheat flour, sugar, eggs, butter\nAllergen Warnings: 1\n  - peanuts + wheat: Peanuts and wheat can cause severe reactions in some people\n\nRecipe 8 - Ingredients: strawberry, chocolate, cream, sugar, vanilla\nAllergen Warnings: 1\n  - strawberry + chocolate: Strawberry and chocolate may trigger migraine in sensitive individuals\n\nTotal Recipes: 8\nSafe Recipes: 3\nUnsafe Recipes: 5\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def test_allergen_detection(self):\n    \"\"\"Conduct explicit tests for allergen detection functionality\"\"\"\n    print(\"\\n======= EXPLICIT ALLERGEN DETECTION TESTS =======\\n\")\n    \n    # Test cases with expected outcomes\n    test_cases = [\n        {\n            \"ingredients\": \"fish, cheese, pasta, olive oil\",\n            \"expect_warning\": True,\n            \"expected_allergens\": [(\"fish\", \"cheese\")]\n        },\n        {\n            \"ingredients\": \"shrimp, milk, butter, pasta\",\n            \"expect_warning\": True,\n            \"expected_allergens\": [(\"shrimp\", \"milk\"), (\"shellfish\", \"milk\"), (\"shellfish\", \"dairy\")]\n        },\n        {\n            \"ingredients\": \"chicken, rice, vegetables, soy sauce\",\n            \"expect_warning\": False,\n            \"expected_allergens\": []\n        },\n        {\n            \"ingredients\": \"salmon, curd, lemon, rice\",\n            \"expect_warning\": True,\n            \"expected_allergens\": [(\"fish\", \"curd\"), (\"salmon\", \"curd\")]\n        },\n        {\n            \"ingredients\": \"tuna, yogurt, celery, onion\",\n            \"expect_warning\": True,\n            \"expected_allergens\": [(\"fish\", \"yogurt\"), (\"tuna\", \"yogurt\")]\n        },\n        {\n            \"ingredients\": \"strawberry, chocolate, sugar, flour\",\n            \"expect_warning\": True,\n            \"expected_allergens\": [(\"strawberry\", \"chocolate\")]\n        },\n        {\n            \"ingredients\": \"peanuts, wheat flour, eggs, sugar\",\n            \"expect_warning\": True,\n            \"expected_allergens\": [(\"peanuts\", \"wheat\")]\n        }\n    ]\n    \n    # Results tracking\n    test_results = []\n    passed = 0\n    failed = 0\n    \n    # Run each test case\n    for i, test_case in enumerate(test_cases):\n        print(f\"\\nTest {i+1}: {test_case['ingredients']}\")\n        print(f\"Expected warnings: {'Yes' if test_case['expect_warning'] else 'No'}\")\n        \n        # Generate recipe\n        result = self.generate_recipe(test_case['ingredients'])\n        warnings = result.get(\"allergen_warnings\", [])\n        \n        # Extract detected allergen pairs\n        detected_pairs = [(w['ingredients'][0], w['ingredients'][1]) for w in warnings]\n        \n        # Check if warnings match expectations\n        has_warnings = len(warnings) > 0\n        warning_status = \"PASS\" if has_warnings == test_case['expect_warning'] else \"FAIL\"\n        \n        # Check for specific allergen pairs - at least one should match\n        allergen_match = False\n        if test_case['expect_warning']:\n            for expected_pair in test_case['expected_allergens']:\n                if any(all(item in detected for item in expected_pair) or\n                       all(detected[i] == expected_pair[i] for i in range(2))\n                       for detected in detected_pairs):\n                    allergen_match = True\n                    break\n        else:\n            allergen_match = len(detected_pairs) == 0\n            \n        allergen_status = \"PASS\" if allergen_match else \"FAIL\"\n        \n        # Overall test result\n        test_passed = warning_status == \"PASS\" and allergen_status == \"PASS\"\n        if test_passed:\n            passed += 1\n        else:\n            failed += 1\n            \n        test_results.append({\n            \"test_case\": test_case,\n            \"warnings\": warnings,\n            \"warning_status\": warning_status,\n            \"allergen_status\": allergen_status,\n            \"overall\": \"PASS\" if test_passed else \"FAIL\"\n        })\n        \n        # Print results for this test\n        print(f\"Warning detection: {warning_status}\")\n        print(f\"Allergen match: {allergen_status}\")\n        print(f\"Overall test result: {'✅ PASS' if test_passed else '❌ FAIL'}\")\n        \n        # Print the detected allergens\n        if warnings:\n            print(\"Detected allergens:\")\n            for j, warning in enumerate(warnings):\n                print(f\"  - {j+1}: {warning['ingredients'][0]} + {warning['ingredients'][1]}\")\n                print(f\"    Reason: {warning['reason']}\")\n        else:\n            print(\"No allergens detected\")\n            \n        # Show recipe excerpt\n        print(\"\\nRecipe excerpt:\")\n        title = result[\"recipe_dict\"][\"title\"]\n        ing_count = len(result[\"recipe_dict\"][\"ingredients\"])\n        print(f\"[TITLE]: {title}\")\n        print(f\"[INGREDIENTS COUNT]: {ing_count}\")\n        print(\"-\" * 50)\n    \n    # Print summary\n    print(\"\\n======= ALLERGEN DETECTION TEST SUMMARY =======\")\n    print(f\"Total tests: {len(test_cases)}\")\n    print(f\"Passed: {passed} ({passed/len(test_cases)*100:.1f}%)\")\n    print(f\"Failed: {failed} ({failed/len(test_cases)*100:.1f}%)\")\n    \n    # Print failure details if any\n    if failed > 0:\n        print(\"\\nFailed tests:\")\n        for i, result in enumerate(test_results):\n            if result[\"overall\"] == \"FAIL\":\n                test_case = result[\"test_case\"]\n                print(f\"- Test {i+1}: {test_case['ingredients']}\")\n                print(f\"  Expected warnings: {'Yes' if test_case['expect_warning'] else 'No'}\")\n                print(f\"  Detected warnings: {'Yes' if result['warnings'] else 'No'}\")\n                print(f\"  Warning status: {result['warning_status']}\")\n                print(f\"  Allergen status: {result['allergen_status']}\")\n    \n    return test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T07:39:00.106084Z","iopub.execute_input":"2025-04-10T07:39:00.10638Z","iopub.status.idle":"2025-04-10T07:39:00.120542Z","shell.execute_reply.started":"2025-04-10T07:39:00.106358Z","shell.execute_reply":"2025-04-10T07:39:00.119457Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom transformers import FlaxAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom pathlib import Path\n\ndef convert_flax_to_pytorch():\n    print(\"Loading Flax model...\")\n    MODEL_NAME_OR_PATH = \"flax-community/t5-recipe-generation\"\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, use_fast=True)\n    \n    print(\"Converting to PyTorch...\")\n    # Convert to PyTorch\n    pt_model = AutoModelForSeq2SeqLM.from_pretrained(\n        MODEL_NAME_OR_PATH, \n        from_flax=True,\n    )\n    \n    # Save the PyTorch model\n    output_dir = Path(\"./optimized_recipe_model_pt\")\n    output_dir.mkdir(exist_ok=True)\n    pt_model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"PyTorch model saved to {output_dir}\")\n    \n    return pt_model, tokenizer\n\ndef optimize_model(model):\n    print(\"Optimizing model...\")\n    # Move to half precision to reduce memory usage and increase speed\n    model = model.half()\n    \n    # Enable torch inference optimizations\n    torch.set_grad_enabled(False)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    \n    return model\n\ndef create_optimized_pipeline():\n    # Load or create the model\n    try:\n        model_path = \"./optimized_recipe_model_pt\"\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    except:\n        model, tokenizer = convert_flax_to_pytorch()\n    \n    # Optimize the model\n    model = optimize_model(model)\n    \n    # Optimized generation parameters\n    generation_kwargs = {\n        \"max_length\": 512,\n        \"min_length\": 64,\n        \"no_repeat_ngram_size\": 3,\n        \"num_beams\": 4,\n        \"early_stopping\": True,\n        \"length_penalty\": 1.2,\n        \"do_sample\": True,  # Enable sampling\n        \"temperature\": 0.8,  # Now properly used with do_sample=True\n        \"top_k\": 50,        # Add top_k sampling\n        \"top_p\": 0.95       # Add nucleus sampling\n    }\n    \n    # Define tokens_map for post-processing\n    special_tokens = tokenizer.all_special_tokens\n    tokens_map = {\n        \"<sep>\": \"--\",\n        \"<section>\": \"\\n\"\n    }\n    \n    def generate_recipes(ingredient_lists, batch_size=4):\n        \"\"\"\n        Generate recipes from multiple ingredient lists efficiently\n        \"\"\"\n        all_recipes = []\n        \n        # Process in batches for efficiency\n        for i in range(0, len(ingredient_lists), batch_size):\n            batch = ingredient_lists[i:i+batch_size]\n            \n            # Prepare inputs\n            prefix = \"items: \"\n            inputs = [prefix + inp for inp in batch]\n            encoded_inputs = tokenizer(\n                inputs,\n                max_length=256,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n            \n            # Move inputs to same device as model\n            if torch.cuda.is_available():\n                encoded_inputs = {k: v.cuda() for k, v in encoded_inputs.items()}\n            \n            # Generate with optimized parameters\n            with torch.no_grad():\n                output_ids = model.generate(\n                    **encoded_inputs,\n                    **generation_kwargs\n                )\n            \n            # Decode and post-process\n            generated_recipes = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n            \n            # Post-process recipes\n            for text in generated_recipes:\n                # Skip special tokens\n                for token in special_tokens:\n                    text = text.replace(token, \"\")\n                \n                # Replace mapped tokens\n                for k, v in tokens_map.items():\n                    text = text.replace(k, v)\n                \n                all_recipes.append(text)\n        \n        return all_recipes\n\n    return generate_recipes\n\ndef optimize_recipe_model():\n    \"\"\"Main function to optimize the T5 recipe model\"\"\"\n    print(\"Starting model optimization process...\")\n    \n    # Create optimized pipeline\n    generate_recipes = create_optimized_pipeline()\n    \n    # Test the optimized model\n    test_ingredients = [\n        \"macaroni, butter, salt, bacon, milk, flour, pepper, cream corn\",\n        \"provolone cheese, bacon, bread, ginger\"\n    ]\n    \n    print(\"\\nTesting optimized model with sample ingredients...\")\n    generated_recipes = generate_recipes(test_ingredients)\n    \n    for i, text in enumerate(generated_recipes):\n        print(f\"\\nRecipe {i+1} from ingredients: {test_ingredients[i]}\")\n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            if section.startswith(\"title:\"):\n                section = section.replace(\"title:\", \"\")\n                headline = \"TITLE\"\n            elif section.startswith(\"ingredients:\"):\n                section = section.replace(\"ingredients:\", \"\")\n                headline = \"INGREDIENTS\"\n            elif section.startswith(\"directions:\"):\n                section = section.replace(\"directions:\", \"\")\n                headline = \"DIRECTIONS\"\n\n            if headline == \"TITLE\":\n                print(f\"[{headline}]: {section.strip().capitalize()}\")\n            else:\n                section_info = [f\"  - {i+1}: {info.strip().capitalize()}\" for i, info in enumerate(section.split(\"--\"))]\n                print(f\"[{headline}]:\")\n                print(\"\\n\".join(section_info))\n\n        print(\"-\" * 50)\n    \n    print(\"\\nModel optimization complete!\")\n    print(\"The optimized model is ready for integration with the Flask/FastAPI backend.\")\n    \n    return generate_recipes\n\nif __name__ == \"__main__\":\n    optimize_recipe_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T07:39:16.322172Z","iopub.execute_input":"2025-04-10T07:39:16.322509Z","iopub.status.idle":"2025-04-10T07:39:48.803281Z","shell.execute_reply.started":"2025-04-10T07:39:16.322464Z","shell.execute_reply":"2025-04-10T07:39:48.80251Z"}},"outputs":[{"name":"stdout","text":"Starting model optimization process...\nLoading Flax model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"flax_model.msgpack:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa13278dc8d84f18a859d980e14d4e9f"}},"metadata":{}},{"name":"stdout","text":"Converting to PyTorch...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_flax_pytorch_utils.py:459: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  pt_model_dict[flax_key] = torch.from_numpy(flax_tensor)\nAll Flax model weights were used when initializing T5ForConditionalGeneration.\n\nSome weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['encoder.embed_tokens.weight', 'lm_head.weight', 'decoder.embed_tokens.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PyTorch model saved to optimized_recipe_model_pt\nOptimizing model...\n\nTesting optimized model with sample ingredients...\n\nRecipe 1 from ingredients: macaroni, butter, salt, bacon, milk, flour, pepper, cream corn\n[TITLE]: Macaroni and corn casserole\n[INGREDIENTS]:\n  - 1: 1 lb. box elbow or elbow pasta\n  - 2: 1/4 c. butter\n  - 3: 1 tsp. salt\n  - 4: 6 slices bacon, cooked and crumbled\n  - 5: 2 1/2 c milk\n  - 6: 2 tbsp flour\n  - 7: 1/8 t. pepper\n  - 8: 1 can cream corn\n[DIRECTIONS]:\n  - 1: Cook pasta according to package directions.\n  - 2: Drain.\n  - 3: In a saucepan, melt butter.\n  - 4: Stir in flour, salt and pepper.\n  - 5: Gradually stir in milk.\n  - 6: Cook and stir until thickened and bubbly.\n  - 7: Add corn and bacon.\n  - 8: Pour into a greased 2 quart casserole.\n  - 9: Bake at 350 degrees for 30 minutes.\n--------------------------------------------------\n\nRecipe 2 from ingredients: provolone cheese, bacon, bread, ginger\n[TITLE]: Grilled provolone and bacon sandwich\n[INGREDIENTS]:\n  - 1: 1 slice provalone cheese\n  - 2: 2 slices bacon\n  - 3: 1 slice sourdough bread\n  - 4: 1 teaspoon pickled ginger\n[DIRECTIONS]:\n  - 1: Preheat a skillet over medium heat.\n  - 2: Add the bacon to the skillet and cook until crispy, about 5 minutes.\n  - 3: Remove the bacon from the pan and drain on a paper towel lined plate.\n  - 4: Place the cheese on one slice of bread and top with the bacon.\n  - 5: Top the bacon with a slice of cheese and a pinch of ginger.\n  - 6: Cover the sandwich with the remaining bread slice.\n  - 7: Cook the sandwich in the skillet until the cheese is melted and the bread is golden brown, about 3 minutes.\n--------------------------------------------------\n\nModel optimization complete!\nThe optimized model is ready for integration with the Flask/FastAPI backend.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom transformers import FlaxAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom pathlib import Path\n\ndef convert_flax_to_pytorch():\n    print(\"Loading Flax model...\")\n    MODEL_NAME_OR_PATH = \"flax-community/t5-recipe-generation\"\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, use_fast=True)\n    \n    print(\"Converting to PyTorch...\")\n    # Convert to PyTorch\n    pt_model = AutoModelForSeq2SeqLM.from_pretrained(\n        MODEL_NAME_OR_PATH, \n        from_flax=True,\n    )\n    \n    # Save the PyTorch model\n    output_dir = Path(\"./optimized_recipe_model_pt\")\n    output_dir.mkdir(exist_ok=True)\n    pt_model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"PyTorch model saved to {output_dir}\")\n    \n    return pt_model, tokenizer\n\ndef optimize_model(model):\n    print(\"Optimizing model...\")\n    # Move to half precision to reduce memory usage and increase speed\n    model = model.half()\n    \n    # Enable torch inference optimizations\n    torch.set_grad_enabled(False)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    \n    return model\n\ndef create_optimized_pipeline():\n    # Load or create the model\n    try:\n        model_path = \"./optimized_recipe_model_pt\"\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    except:\n        model, tokenizer = convert_flax_to_pytorch()\n    \n    # Optimize the model\n    model = optimize_model(model)\n    \n    # Optimized generation parameters\n    generation_kwargs = {\n        \"max_length\": 512,\n        \"min_length\": 64,\n        \"no_repeat_ngram_size\": 3,\n        \"num_beams\": 4,\n        \"early_stopping\": True,\n        \"length_penalty\": 1.2,\n        \"do_sample\": True,  # Enable sampling\n        \"temperature\": 0.8,  # Now properly used with do_sample=True\n        \"top_k\": 50,        # Add top_k sampling\n        \"top_p\": 0.95       # Add nucleus sampling\n    }\n    \n    # Define tokens_map for post-processing\n    special_tokens = tokenizer.all_special_tokens\n    tokens_map = {\n        \"<sep>\": \"--\",\n        \"<section>\": \"\\n\"\n    }\n    \n    def generate_recipes(ingredient_lists, batch_size=4):\n        \"\"\"\n        Generate recipes from multiple ingredient lists efficiently\n        \"\"\"\n        all_recipes = []\n        \n        # Process in batches for efficiency\n        for i in range(0, len(ingredient_lists), batch_size):\n            batch = ingredient_lists[i:i+batch_size]\n            \n            # Prepare inputs\n            prefix = \"items: \"\n            inputs = [prefix + inp for inp in batch]\n            encoded_inputs = tokenizer(\n                inputs,\n                max_length=256,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n            \n            # Move inputs to same device as model\n            if torch.cuda.is_available():\n                encoded_inputs = {k: v.cuda() for k, v in encoded_inputs.items()}\n            \n            # Generate with optimized parameters\n            with torch.no_grad():\n                output_ids = model.generate(\n                    **encoded_inputs,\n                    **generation_kwargs\n                )\n            \n            # Decode and post-process\n            generated_recipes = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n            \n            # Post-process recipes\n            for text in generated_recipes:\n                # Skip special tokens\n                for token in special_tokens:\n                    text = text.replace(token, \"\")\n                \n                # Replace mapped tokens\n                for k, v in tokens_map.items():\n                    text = text.replace(k, v)\n                \n                all_recipes.append(text)\n        \n        return all_recipes\n\n    return generate_recipes\n\ndef optimize_recipe_model():\n    \"\"\"Main function to optimize the T5 recipe model\"\"\"\n    print(\"Starting model optimization process...\")\n    \n    # Create optimized pipeline\n    generate_recipes = create_optimized_pipeline()\n    \n    # Test the optimized model\n    test_ingredients = [\n        \"paneer, butter, peas, fresh cream\",\n        \"curd, fish\"\n    ]\n    \n    print(\"\\nTesting optimized model with sample ingredients...\")\n    generated_recipes = generate_recipes(test_ingredients)\n    \n    for i, text in enumerate(generated_recipes):\n        print(f\"\\nRecipe {i+1} from ingredients: {test_ingredients[i]}\")\n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            if section.startswith(\"title:\"):\n                section = section.replace(\"title:\", \"\")\n                headline = \"TITLE\"\n            elif section.startswith(\"ingredients:\"):\n                section = section.replace(\"ingredients:\", \"\")\n                headline = \"INGREDIENTS\"\n            elif section.startswith(\"directions:\"):\n                section = section.replace(\"directions:\", \"\")\n                headline = \"DIRECTIONS\"\n\n            if headline == \"TITLE\":\n                print(f\"[{headline}]: {section.strip().capitalize()}\")\n            else:\n                section_info = [f\"  - {i+1}: {info.strip().capitalize()}\" for i, info in enumerate(section.split(\"--\"))]\n                print(f\"[{headline}]:\")\n                print(\"\\n\".join(section_info))\n\n        print(\"-\" * 50)\n    \n    print(\"\\nModel optimization complete!\")\n    print(\"The optimized model is ready for integration with the Flask/FastAPI backend.\")\n    \n    return generate_recipes\n\nif __name__ == \"__main__\":\n    optimize_recipe_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T07:40:03.062143Z","iopub.execute_input":"2025-04-10T07:40:03.062442Z","iopub.status.idle":"2025-04-10T07:40:07.780733Z","shell.execute_reply.started":"2025-04-10T07:40:03.06242Z","shell.execute_reply":"2025-04-10T07:40:07.779784Z"}},"outputs":[{"name":"stdout","text":"Starting model optimization process...\nOptimizing model...\n\nTesting optimized model with sample ingredients...\n\nRecipe 1 from ingredients: paneer, butter, peas, fresh cream\n[TITLE]: Paneer with peas and cream\n[INGREDIENTS]:\n  - 1: 1 lb. pkg. samosas or sourdough bread\n  - 2: 8 oz. drained, cubed panereer\n  - 3: 2 tbsp. butter\n  - 4: 1 1/2 c. cooked, shelled, peeled and mashed boiled potatoes\n  - 5: 1 pt. fresh cream\n[DIRECTIONS]:\n  - 1: Preheat oven to 350 .\n  - 2: Spread bread cubes in a single layer on a baking sheet.\n  - 3: Bake until lightly browned, about 10 minutes.\n  - 4: Remove from oven and set aside.\n--------------------------------------------------\n\nRecipe 2 from ingredients: curd, fish\n[TITLE]: Chinese style bean curd and fish\n[INGREDIENTS]:\n  - 1: 1 block beancurd\n  - 2: 1 tbsp fish stock\n[DIRECTIONS]:\n  - 1: Cut the bean curde into bite sized pieces.\n  - 2: Put the beancurde and fish stock in a frying pan and bring to a boil.\n  - 3: When it starts to boil, turn the heat down to low and simmer for about 10 minutes.\n  - 4: Turn off the heat and serve.\n--------------------------------------------------\n\nModel optimization complete!\nThe optimized model is ready for integration with the Flask/FastAPI backend.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom transformers import FlaxAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom pathlib import Path\n\ndef convert_flax_to_pytorch():\n    print(\"Loading Flax model...\")\n    MODEL_NAME_OR_PATH = \"flax-community/t5-recipe-generation\"\n    flax_model = FlaxAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, use_fast=True)\n    \n    print(\"Converting to PyTorch...\")\n    # Convert to PyTorch\n    pt_model = AutoModelForSeq2SeqLM.from_pretrained(\n        MODEL_NAME_OR_PATH, \n        from_flax=True,\n    )\n    \n    # Save the PyTorch model\n    output_dir = Path(\"./optimized_recipe_model_pt\")\n    output_dir.mkdir(exist_ok=True)\n    pt_model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"PyTorch model saved to {output_dir}\")\n    \n    return pt_model, tokenizer\n\ndef optimize_model(model):\n    print(\"Optimizing model...\")\n    # Move to half precision to reduce memory usage and increase speed\n    model = model.half()\n    \n    # Enable torch inference optimizations\n    torch.set_grad_enabled(False)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    \n    return model\n\ndef create_optimized_pipeline():\n    # Load or create the model\n    try:\n        model_path = \"./optimized_recipe_model_pt\"\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n    except:\n        model, tokenizer = convert_flax_to_pytorch()\n    \n    # Optimize the model\n    model = optimize_model(model)\n    \n    # Optimized generation parameters\n    generation_kwargs = {\n        \"max_length\": 512,\n        \"min_length\": 64,\n        \"no_repeat_ngram_size\": 3,\n        \"num_beams\": 4,\n        \"early_stopping\": True,\n        \"length_penalty\": 1.2,\n        \"do_sample\": True,  # Enable sampling\n        \"temperature\": 0.8,  # Now properly used with do_sample=True\n        \"top_k\": 50,        # Add top_k sampling\n        \"top_p\": 0.95       # Add nucleus sampling\n    }\n    \n    # Define tokens_map for post-processing\n    special_tokens = tokenizer.all_special_tokens\n    tokens_map = {\n        \"<sep>\": \"--\",\n        \"<section>\": \"\\n\"\n    }\n    \n    def generate_recipes(ingredient_lists, batch_size=4):\n        \"\"\"\n        Generate recipes from multiple ingredient lists efficiently\n        \"\"\"\n        all_recipes = []\n        \n        # Process in batches for efficiency\n        for i in range(0, len(ingredient_lists), batch_size):\n            batch = ingredient_lists[i:i+batch_size]\n            \n            # Prepare inputs\n            prefix = \"items: \"\n            inputs = [prefix + inp for inp in batch]\n            encoded_inputs = tokenizer(\n                inputs,\n                max_length=256,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n            \n            # Move inputs to same device as model\n            if torch.cuda.is_available():\n                encoded_inputs = {k: v.cuda() for k, v in encoded_inputs.items()}\n            \n            # Generate with optimized parameters\n            with torch.no_grad():\n                output_ids = model.generate(\n                    **encoded_inputs,\n                    **generation_kwargs\n                )\n            \n            # Decode and post-process\n            generated_recipes = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n            \n            # Post-process recipes\n            for text in generated_recipes:\n                # Skip special tokens\n                for token in special_tokens:\n                    text = text.replace(token, \"\")\n                \n                # Replace mapped tokens\n                for k, v in tokens_map.items():\n                    text = text.replace(k, v)\n                \n                all_recipes.append(text)\n        \n        return all_recipes\n\n    return generate_recipes\n\ndef optimize_recipe_model():\n    \"\"\"Main function to optimize the T5 recipe model\"\"\"\n    print(\"Starting model optimization process...\")\n    \n    # Create optimized pipeline\n    generate_recipes = create_optimized_pipeline()\n    \n    # Test the optimized model\n    test_ingredients = [\n        \"flour, sugar, vanilla extract, butter, cream cheese, chocolate chips\",\n        \"brinjal, channa, drumstick,tomato, onion\"\n    ]\n    \n    print(\"\\nTesting optimized model with sample ingredients...\")\n    generated_recipes = generate_recipes(test_ingredients)\n    \n    for i, text in enumerate(generated_recipes):\n        print(f\"\\nRecipe {i+1} from ingredients: {test_ingredients[i]}\")\n        sections = text.split(\"\\n\")\n        for section in sections:\n            section = section.strip()\n            if section.startswith(\"title:\"):\n                section = section.replace(\"title:\", \"\")\n                headline = \"TITLE\"\n            elif section.startswith(\"ingredients:\"):\n                section = section.replace(\"ingredients:\", \"\")\n                headline = \"INGREDIENTS\"\n            elif section.startswith(\"directions:\"):\n                section = section.replace(\"directions:\", \"\")\n                headline = \"DIRECTIONS\"\n\n            if headline == \"TITLE\":\n                print(f\"[{headline}]: {section.strip().capitalize()}\")\n            else:\n                section_info = [f\"  - {i+1}: {info.strip().capitalize()}\" for i, info in enumerate(section.split(\"--\"))]\n                print(f\"[{headline}]:\")\n                print(\"\\n\".join(section_info))\n\n        print(\"-\" * 50)\n    \n    print(\"\\nModel optimization complete!\")\n    print(\"The optimized model is ready for integration with the Flask/FastAPI backend.\")\n    \n    return generate_recipes\n\nif __name__ == \"__main__\":\n    optimize_recipe_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T07:40:23.617132Z","iopub.execute_input":"2025-04-10T07:40:23.617458Z","iopub.status.idle":"2025-04-10T07:40:29.593289Z","shell.execute_reply.started":"2025-04-10T07:40:23.617431Z","shell.execute_reply":"2025-04-10T07:40:29.592564Z"}},"outputs":[{"name":"stdout","text":"Starting model optimization process...\nOptimizing model...\n\nTesting optimized model with sample ingredients...\n\nRecipe 1 from ingredients: flour, sugar, vanilla extract, butter, cream cheese, chocolate chips\n[TITLE]: Chocolate chip cream cheese cookies\n[INGREDIENTS]:\n  - 1: 2 c. flour\n  - 2: 1 c sugar\n  - 3: 2 tsp. vanilla extract\n  - 4: 2 sticks butter, softened\n  - 5: 1 8 oz. pkg. cream cheese\n  - 6: 1 6 ox. bag chocolate chips\n[DIRECTIONS]:\n  - 1: Mix flour, sugar, vanilla, butter and cream cheese.\n  - 2: Stir in chocolate chips.\n  - 3: Drop by teaspoonfuls onto ungreased cookie sheet.\n  - 4: Bake at 350 for 10 to 12 minutes.\n--------------------------------------------------\n\nRecipe 2 from ingredients: brinjal, channa, drumstick,tomato, onion\n[TITLE]: Brinjal, channa, and drumstick stew\n[INGREDIENTS]:\n  - 1: 1 medium size idaho or a smoky kashmiri brinjala\n  - 2: 1 medium sized iranian kishma or chana dal\n  - 3: 1/2 medium size drumstick\n  - 4: 1 large ripe tomato\n  - 5: 1 small onion\n[DIRECTIONS]:\n  - 1: Cut the kushma into 1 inch cubes.\n  - 2: Peel the onion and cut it into thin slices.\n  - 3: Cut the tomato in half and remove the seeds.\n  - 4: Put all the ingredients into a large pot.\n  - 5: Cover with water and bring to a boil.\n  - 6: Lower the heat and simmer for about 30 minutes, or until the vegetables are tender.\n  - 7: Serve hot.\n--------------------------------------------------\n\nModel optimization complete!\nThe optimized model is ready for integration with the Flask/FastAPI backend.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"> 06-04-2025","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    EarlyStoppingCallback,\n    DataCollatorForSeq2Seq,\n    T5TokenizerFast # Can be useful but AutoTokenizer usually suffices\n)\nfrom datasets import Dataset, DatasetDict, load_from_disk\nimport os\nimport json\nimport random\nimport time\n\n# --- Configuration ---\nBASE_MODEL = \"t5-base\"\nSYNTHETIC_DATA_DIR = \"synthetic_recipe_data\"\nRAW_DATASET_DIR = \"recipe_dataset\"\nPREPROCESSED_DATA_DIR = \"preprocessed_recipe_dataset\"\nOUTPUT_MODEL_DIR = \"./recipe_model_finetuned\" # Fine-tuning output / Trained model location\nLOGGING_DIR = \"./recipe_logs\" # Fine-tuning logs\n\n# Fine-tuning Parameters\nSAMPLE_SIZE = 2000 # Reduced for faster demo; increase for better results (e.g., 10000)\nNUM_TRAIN_EPOCHS = 1 # Reduced for faster demo; increase for better results (e.g., 3)\nBATCH_SIZE = 4 # Adjust based on GPU memory\nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0.01\nWARMUP_RATIO = 0.1\nFP16 = torch.cuda.is_available() # Enable FP16 if GPU is available\n\n# Set seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# --- Stage 1: Data Generation ---\n\ndef create_recipe_dataset(sample_size=1000, seed=42):\n    \"\"\"\n    Create a compatible dataset for T5 recipe generation.\n    Uses sample data formatted to match the expected input/output structure.\n    \"\"\"\n    print(\"\\n--- Stage 1: Creating Synthetic Recipe Dataset ---\")\n    print(f\"Generating {sample_size} synthetic recipes...\")\n\n    # Set seeds for reproducibility within function scope if needed again\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Sample ingredients for synthetic data\n    common_ingredients = [\n        \"chicken\", \"beef\", \"pork\", \"salmon\", \"tuna\", \"shrimp\", \"lamb\", \"cod\", \"haddock\", \"mackerel\", # Fish/Meat\n        \"pasta\", \"rice\", \"potatoes\", \"bread\", \"flour\", \"oats\", \"quinoa\", \"couscous\", \"noodles\", # Grains/Starch\n        \"onion\", \"garlic\", \"tomatoes\", \"bell peppers\", \"carrots\", \"broccoli\", \"cauliflower\",\n        \"spinach\", \"lettuce\", \"mushrooms\", \"zucchini\", \"eggplant\", \"corn\", \"peas\", \"beans\", # Vegetables\n        \"apple\", \"banana\", \"orange\", \"lemon\", \"lime\", \"berries\", \"mango\", \"pineapple\", # Fruits\n        \"butter\", \"olive oil\", \"vegetable oil\", \"coconut oil\", \"sesame oil\", # Fats/Oils\n        \"salt\", \"pepper\", \"oregano\", \"basil\", \"thyme\", \"rosemary\", \"cumin\", \"coriander\", \"paprika\", \"chili powder\", # Spices\n        \"milk\", \"cream\", \"cheese\", \"cheddar\", \"parmesan\", \"mozzarella\", \"yogurt\", \"curd\", \"eggs\", \"mayonnaise\", # Dairy/Binders\n        \"sugar\", \"brown sugar\", \"honey\", \"maple syrup\", # Sweeteners\n        \"lentils\", \"chickpeas\", \"tofu\", \"tempeh\", # Legumes/Plant-protein\n        \"chocolate\", \"vanilla extract\", \"cinnamon\", \"nutmeg\", \"soy sauce\", \"vinegar\", \"mustard\" # Flavorings\n    ]\n\n    # Create directory for generated data\n    os.makedirs(SYNTHETIC_DATA_DIR, exist_ok=True)\n\n    # Generate synthetic recipes\n    recipes = []\n    for i in range(sample_size):\n        num_ingredients = random.randint(4, 12)\n        ingredients = random.sample(common_ingredients, num_ingredients)\n\n        main_ingredient = random.choice(ingredients)\n        cooking_methods = [\"Roasted\", \"Grilled\", \"Baked\", \"Fried\", \"Steamed\", \"Sautéed\", \"Slow-cooked\", \"Spicy\", \"Creamy\", \"Simple\", \"Quick\"]\n        dish_types = [\"Casserole\", \"Soup\", \"Stew\", \"Salad\", \"Pasta\", \"Curry\", \"Stir-fry\", \"Sandwich\", \"Bowl\", \"Tacos\", \"Pizza\"]\n        title = f\"{random.choice(cooking_methods)} {main_ingredient.capitalize()} {random.choice(dish_types)}\"\n\n        directions = [\n            f\"Prepare all ingredients: Chop vegetables, measure spices.\",\n            f\"Preheat your {random.choice(['oven', 'skillet', 'grill', 'pot'])} to {random.randint(150, 220)}°C ({random.randint(300, 425)}°F).\",\n            f\"In a bowl, combine {ingredients[0]} and {ingredients[1]}.\",\n            f\"Add {random.choice(['olive oil', 'butter'])} to a {random.choice(['pan', 'pot'])} over medium heat.\",\n            f\"Sauté {random.choice(['onion', 'garlic'])} until fragrant, about {random.randint(2, 5)} minutes.\",\n            f\"Add {', '.join(ingredients[2:min(5, len(ingredients))])} and cook for {random.randint(5, 15)} minutes, stirring occasionally.\",\n            f\"Stir in the remaining ingredients ({', '.join(ingredients[min(5, len(ingredients)):])}) and {random.choice(['broth', 'water', 'sauce'])}.\",\n            f\"Bring to a simmer, then reduce heat and cook for {random.randint(10, 30)} minutes until {main_ingredient} is cooked through.\",\n            f\"Season with salt, pepper, and other desired spices to taste.\",\n            f\"Garnish with {random.choice(['parsley', 'cilantro', 'cheese', 'nuts'])} and serve hot.\"\n        ]\n        random.shuffle(directions) # Make directions less predictable\n        directions = directions[:random.randint(5, 8)] # Use a subset of directions\n\n        input_ingredients = \", \".join(ingredients)\n\n        output_text = (\n            f\"title: {title} <section> \"\n            f\"ingredients: {' <sep> '.join(ingredients)} <section> \"\n            f\"directions: {' <sep> '.join(directions)}\"\n        )\n\n        recipes.append({\n            \"input_text\": f\"items: {input_ingredients}\",\n            \"output_text\": output_text\n        })\n\n    # Save raw synthetic data (optional, but good for reference)\n    output_file = os.path.join(SYNTHETIC_DATA_DIR, \"synthetic_recipes.json\")\n    with open(output_file, 'w') as f:\n        json.dump(recipes, f, indent=2)\n    print(f\"Saved raw synthetic data to {output_file}\")\n\n    # Create dataset splits\n    df = pd.DataFrame(recipes)\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n    train_size = int(0.8 * len(df))\n    val_size = int(0.1 * len(df))\n\n    train_df = df[:train_size]\n    val_df = df[train_size:train_size+val_size]\n    test_df = df[train_size+val_size:]\n\n    dataset_dict = DatasetDict({\n        'train': Dataset.from_pandas(train_df),\n        'validation': Dataset.from_pandas(val_df),\n        'test': Dataset.from_pandas(test_df)\n    })\n\n    # Save dataset using the defined path constant\n    dataset_dict.save_to_disk(RAW_DATASET_DIR)\n    print(f\"Dataset created with {len(df)} recipes and saved to '{RAW_DATASET_DIR}'\")\n    print(f\"Split sizes -> Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n\n    # Display a few examples\n    print(\"\\nExample recipes (first 3):\")\n    for i in range(min(3, len(df))):\n        print(f\"\\nInput: {df.iloc[i]['input_text']}\")\n        print(f\"Output: {df.iloc[i]['output_text']}\")\n\n    return dataset_dict\n\n# --- Stage 2: Data Preprocessing ---\n\ndef preprocess_recipe_data(\n    raw_dataset_dir=RAW_DATASET_DIR,\n    preprocessed_dataset_dir=PREPROCESSED_DATA_DIR,\n    model_name=BASE_MODEL,\n    max_input_length=256,\n    max_output_length=512\n    ):\n    \"\"\"\n    Preprocess recipe dataset for sequence-to-sequence training with T5.\n    \"\"\"\n    print(\"\\n--- Stage 2: Preprocessing Recipe Dataset ---\")\n\n    # Load dataset\n    try:\n        dataset = load_from_disk(raw_dataset_dir)\n        print(f\"Loaded raw dataset from '{raw_dataset_dir}'\")\n    except FileNotFoundError:\n        print(f\"Error: Raw dataset directory '{raw_dataset_dir}' not found.\")\n        print(\"Please ensure Stage 1 (create_recipe_dataset) ran successfully.\")\n        return None, None\n\n    # Load tokenizer\n    print(f\"Loading tokenizer '{model_name}'...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\n    # Add special tokens for recipe formatting\n    special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n    num_added_toks = tokenizer.add_special_tokens(special_tokens)\n    if num_added_toks > 0:\n        print(f\"Added {num_added_toks} special tokens ('<sep>', '<section>') to the tokenizer.\")\n    else:\n        print(\"Special tokens already present in the tokenizer.\")\n\n    def preprocess_function(examples):\n        # Ensure examples contain the expected keys\n        if \"input_text\" not in examples or \"output_text\" not in examples:\n             print(\"Warning: Missing 'input_text' or 'output_text' in examples batch.\")\n             # Handle potential list of dicts vs dict of lists\n             input_texts = examples.get(\"input_text\", [])\n             output_texts = examples.get(\"output_text\", [])\n        else:\n             input_texts = examples[\"input_text\"]\n             output_texts = examples[\"output_text\"]\n\n        # Tokenize inputs\n        model_inputs = tokenizer(\n            input_texts,\n            max_length=max_input_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\" # Request PyTorch tensors\n        )\n\n        # Tokenize outputs (labels)\n        # Important: T5 uses input_ids as labels during training\n        labels = tokenizer(\n            text_target=output_texts, # Use text_target for T5 labels\n            max_length=max_output_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\" # Request PyTorch tensors\n        )\n\n        # Replace padding token id in labels with -100 for loss calculation\n        label_input_ids = labels[\"input_ids\"]\n        label_input_ids[label_input_ids == tokenizer.pad_token_id] = -100\n\n        model_inputs[\"labels\"] = label_input_ids\n\n        # Detach tensors before returning from map function if needed (usually not necessary)\n        # model_inputs = {k: v.squeeze().tolist() for k,v in model_inputs.items()}\n        # Convert tensors back to lists for dataset saving if not using set_format\n        model_inputs = {k: v.squeeze().tolist() for k, v in model_inputs.items()}\n\n        return model_inputs\n\n    # Apply preprocessing to all splits\n    print(\"Applying preprocessing function...\")\n    processed_dataset = dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=dataset[\"train\"].column_names # Remove original text columns\n    )\n\n    # Set format to PyTorch tensors for DataLoader\n    processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n    # Save processed dataset\n    processed_dataset.save_to_disk(preprocessed_dataset_dir)\n    print(f\"Preprocessed dataset saved to '{preprocessed_dataset_dir}'\")\n\n    # Optional: Check a sample\n    print(\"\\nSample preprocessed example (first 10 tokens):\")\n    sample = processed_dataset[\"train\"][0]\n    print(f\"Input IDs: {sample['input_ids'][:10].tolist()}\") # Convert tensor to list for printing\n    print(f\"Attention mask: {sample['attention_mask'][:10].tolist()}\")\n    print(f\"Labels: {sample['labels'][:10].tolist()}\")\n\n\n    return processed_dataset, tokenizer\n\n# --- Post-Processor Definition --- (Single definition)\n\nclass RecipePostProcessor:\n    def __init__(self, tokenizer):\n        \"\"\"\n        Initialize post-processor with tokenizer for handling special tokens.\n        Includes allergen detection and additional recipe processing.\n        \"\"\"\n        self.tokenizer = tokenizer\n        # Get all special tokens EXCEPT pad, bos, eos if needed for cleaning,\n        # but focus on removing the generation control tokens and keeping our custom ones.\n        all_special_ids = tokenizer.all_special_ids\n        self.special_tokens_to_remove = [t for t_id in all_special_ids for t in tokenizer.convert_ids_to_tokens(t_id, skip_special_tokens=False) if t not in ['<sep>', '<section>', tokenizer.eos_token, tokenizer.bos_token, tokenizer.pad_token]] # keep structure tokens + EOS/PAD\n        # Explicitly add common control tokens if not in special_ids list\n        self.special_tokens_to_remove.extend(['<s>', '</s>', '<pad>'])\n        self.special_tokens_to_remove = list(set(self.special_tokens_to_remove)) # Unique\n\n        self.tokens_map = {\n            \"<sep>\": \"--\",      # Use for separating items within a section\n            \"<section>\": \"\\n\",    # Use for separating sections (title, ingredients, directions)\n            tokenizer.eos_token: \"\" # Remove End Of Sequence token\n        }\n\n        # Enhanced Allergen combinations to avoid - Be mindful of keywords\n        self.allergen_combinations = [\n            (\"fish\", \"dairy\", \"Potential digestive issues or sensitivities when combining fish and dairy.\"),\n            (\"fish\", \"yogurt\", \"Potential digestive issues or sensitivities.\"),\n            (\"fish\", \"milk\", \"Potential digestive issues or sensitivities.\"),\n            (\"fish\", \"curd\", \"Potential digestive issues or sensitivities.\"),\n            (\"fish\", \"cheese\", \"Potential digestive issues or sensitivities (especially certain types).\"),\n            (\"tuna\", \"cheese\", \"Often considered an allergenic or sensitive combination.\"),\n            (\"salmon\", \"cheese\", \"Often considered an allergenic or sensitive combination.\"),\n            (\"shellfish\", \"dairy\", \"Common allergen combination trigger.\"),\n            (\"shellfish\", \"milk\", \"Common allergen combination trigger.\"),\n            (\"shrimp\", \"milk\", \"Common allergen combination trigger.\"),\n            (\"shellfish\", \"cheese\", \"Common allergen combination trigger.\"),\n            (\"peanuts\", \"gluten\", \"Co-occurrence risk for individuals with multiple allergies (e.g., celiac). Needs context.\"),\n            (\"peanuts\", \"wheat\", \"Co-occurrence risk. Needs context.\"),\n            (\"shellfish\", \"mango\", \"Potential histamine reaction trigger in sensitive individuals.\"),\n            (\"strawberry\", \"chocolate\", \"Potential migraine trigger in sensitive individuals.\")\n            # Add more specific terms if needed: e.g., (\"haddock\", \"cheddar\") etc.\n        ]\n        # Keywords for broad categories to improve matching\n        self.allergen_keywords = {\n             \"fish\": [\"fish\", \"tuna\", \"salmon\", \"cod\", \"haddock\", \"mackerel\", \"sardine\"],\n             \"shellfish\": [\"shellfish\", \"shrimp\", \"prawn\", \"crab\", \"lobster\", \"clam\", \"oyster\", \"mussel\", \"scallop\"],\n             \"dairy\": [\"dairy\", \"milk\", \"cream\", \"cheese\", \"cheddar\", \"parmesan\", \"mozzarella\", \"yogurt\", \"curd\", \"butter\"], # Note: butter has low lactose\n             \"gluten\": [\"gluten\", \"wheat\", \"barley\", \"rye\", \"flour\"], # Be careful with just \"flour\"\n             \"wheat\": [\"wheat\", \"flour\"],\n             # Add others like nuts, soy, eggs if desired\n        }\n\n\n    def postprocess_text(self, generated_texts):\n        \"\"\"\n        Post-process generated recipe texts:\n        1. Decode and clean special tokens.\n        2. Replace mapped tokens (<sep>, <section>) with human-readable versions.\n        3. Format into structured recipes.\n        4. Check for allergen combinations.\n        \"\"\"\n        processed_recipes = []\n\n        for text in generated_texts:\n            # Decode might handle some special tokens, but we clean residual ones\n            # Remove other special tokens like <pad>, potentially <s>, etc.\n            for token in self.special_tokens_to_remove:\n                 if token in text: # Check before replacing\n                    text = text.replace(token, \"\")\n\n            # Replace mapped tokens with readable versions AFTER cleaning others\n            for k, v in self.tokens_map.items():\n                 if k in text: # Check before replacing\n                    text = text.replace(k, v)\n\n            text = text.strip() # Remove leading/trailing whitespace\n\n            # Format structured recipe\n            formatted_recipe = self._format_recipe(text)\n\n            # Check for allergen combinations\n            allergen_warnings = self.check_allergens(formatted_recipe)\n            formatted_recipe[\"allergen_warnings\"] = allergen_warnings\n\n            processed_recipes.append(formatted_recipe)\n\n        return processed_recipes\n\n    def _format_recipe(self, text):\n        \"\"\"Format recipe text into structured dictionary\"\"\"\n        recipe_dict = {\"title\": \"Untitled Recipe\", \"ingredients\": [], \"directions\": [], \"allergen_warnings\": []}\n        current_section = None\n\n        # Split by the section separator we introduced (\\n)\n        lines = text.split(\"\\n\")\n\n        for line in lines:\n            line = line.strip()\n            if not line: continue # Skip empty lines\n\n            if line.lower().startswith(\"title:\"):\n                recipe_dict[\"title\"] = line[len(\"title:\"):].strip().capitalize()\n                current_section = \"title\"\n            elif line.lower().startswith(\"ingredients:\"):\n                # Handle ingredients possibly spread across the same line after \"ingredients:\" marker\n                items_text = line[len(\"ingredients:\"):].strip()\n                if items_text:\n                     recipe_dict[\"ingredients\"].extend([\n                         item.strip().capitalize() for item in items_text.split(\"--\") if item.strip()\n                     ])\n                current_section = \"ingredients\"\n            elif line.lower().startswith(\"directions:\"):\n                # Handle directions possibly spread across the same line\n                items_text = line[len(\"directions:\"):].strip()\n                if items_text:\n                    recipe_dict[\"directions\"].extend([\n                        item.strip().capitalize() for item in items_text.split(\"--\") if item.strip()\n                    ])\n                current_section = \"directions\"\n            elif current_section == \"ingredients\" and line: # If already in ingredients section, treat line as ingredients\n                 recipe_dict[\"ingredients\"].extend([\n                    item.strip().capitalize() for item in line.split(\"--\") if item.strip()\n                 ])\n            elif current_section == \"directions\" and line: # If already in directions section, treat line as directions\n                 recipe_dict[\"directions\"].extend([\n                    item.strip().capitalize() for item in line.split(\"--\") if item.strip()\n                 ])\n            # else: Handle lines that don't fit expected structure (optional: log warning)\n\n        # Clean up potentially empty items from splitting issues\n        recipe_dict[\"ingredients\"] = [ing for ing in recipe_dict[\"ingredients\"] if ing]\n        recipe_dict[\"directions\"] = [step for step in recipe_dict[\"directions\"] if step]\n\n        # Capitalize first letter of steps\n        recipe_dict[\"directions\"] = [step[0].upper() + step[1:] if step else \"\" for step in recipe_dict[\"directions\"]]\n\n\n        return recipe_dict\n\n    def check_allergens(self, recipe_dict):\n        \"\"\"Check for potentially problematic ingredient combinations using keywords.\"\"\"\n        warnings = []\n        ingredients_lower = [ing.lower() for ing in recipe_dict[\"ingredients\"]]\n        present_keywords = set()\n\n        # Identify all keywords present in the ingredients list\n        for ing_lower in ingredients_lower:\n            for category, keywords in self.allergen_keywords.items():\n                for keyword in keywords:\n                    if keyword in ing_lower:\n                        present_keywords.add(category)\n                        break # Go to next category once a match is found for this ingredient\n\n        # Check defined combinations based on categories found\n        checked_pairs = set()\n        for cat1, cat2, reason in self.allergen_combinations:\n             # Use the broader categories defined in allergen_keywords keys\n             cat1_present = cat1 in present_keywords\n             cat2_present = cat2 in present_keywords\n\n             # Avoid duplicate warnings (e.g., fish-dairy vs dairy-fish)\n             pair = tuple(sorted((cat1, cat2)))\n             if cat1_present and cat2_present and pair not in checked_pairs:\n                 warnings.append({\n                     \"categories\": (cat1, cat2),\n                     \"reason\": reason\n                 })\n                 checked_pairs.add(pair)\n\n        return warnings\n\n    def format_for_display(self, recipe_dict):\n        \"\"\"Format recipe dictionary for readable display\"\"\"\n        if not isinstance(recipe_dict, dict):\n            return \"Invalid recipe format (not a dictionary)\"\n\n        display_text = f\"[TITLE]: {recipe_dict.get('title', 'N/A')}\\n\\n\"\n\n        display_text += \"[INGREDIENTS]:\\n\"\n        ingredients = recipe_dict.get('ingredients', [])\n        if ingredients:\n            for i, ingredient in enumerate(ingredients):\n                display_text += f\"  - {ingredient}\\n\" # No numbering needed, just list\n        else:\n            display_text += \"  (No ingredients listed)\\n\"\n\n        display_text += \"\\n[DIRECTIONS]:\\n\"\n        directions = recipe_dict.get('directions', [])\n        if directions:\n            for i, step in enumerate(directions):\n                display_text += f\"  {i+1}. {step}\\n\" # Number steps\n        else:\n             display_text += \"  (No directions listed)\\n\"\n\n        # Add allergen warnings if present\n        allergen_warnings = recipe_dict.get(\"allergen_warnings\", [])\n        if allergen_warnings:\n            display_text += \"\\n[⚠️ POTENTIAL ALLERGEN/SENSITIVITY WARNINGS]:\\n\"\n            for i, warning in enumerate(allergen_warnings):\n                 cat1, cat2 = warning['categories']\n                 display_text += f\"  - Combination ({cat1.capitalize()} + {cat2.capitalize()}): {warning['reason']}\\n\"\n\n        return display_text\n\n    def evaluate_recipe_quality(self, recipe_dict):\n        \"\"\"Evaluate recipe quality based on simple heuristics (0-100)\"\"\"\n        if not isinstance(recipe_dict, dict): return 0\n        score = 0\n        max_score = 100\n\n        # Check title presence and basic length\n        if recipe_dict.get(\"title\", \"\") and len(recipe_dict[\"title\"]) > 3 and recipe_dict[\"title\"] != \"Untitled Recipe\":\n            score += 15 # More points for a seemingly valid title\n        else:\n             score += 5 # Minimal points if title is missing/default\n\n        # Check ingredients presence and quantity\n        ingredients = recipe_dict.get(\"ingredients\", [])\n        if ingredients:\n            score += min(len(ingredients) * 4, 30)  # Points per ingredient, capped\n\n            # Check ingredient variety (crude check)\n            unique_words = set()\n            for ingredient in ingredients:\n                unique_words.update(ingredient.lower().split())\n            score += min(len(unique_words) * 1, 15) # Points for unique words, capped\n        else:\n            score -= 10 # Penalize missing ingredients\n\n        # Check directions presence and quantity/detail\n        directions = recipe_dict.get(\"directions\", [])\n        if directions:\n            score += min(len(directions) * 5, 30)  # Points per step, capped\n\n            # Check step length as proxy for detail\n            total_length = sum(len(step) for step in directions)\n            score += min(total_length // 30, 10) # Points for total length, capped\n        else:\n             score -= 10 # Penalize missing directions\n\n        # Deduct significant points for allergen warnings\n        allergen_warnings = recipe_dict.get(\"allergen_warnings\", [])\n        score -= len(allergen_warnings) * 20  # Heavy penalty per warning\n\n        # Normalize score to be between 0 and 100\n        normalized_score = max(0, min(score, max_score))\n\n        return int(normalized_score) # Return integer score\n\n\n# --- Stage 3: Fine-Tuning ---\n\ndef fine_tune_recipe_model(\n    base_model=BASE_MODEL,\n    output_dir=OUTPUT_MODEL_DIR,\n    preprocessed_dataset_dir=PREPROCESSED_DATA_DIR,\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    warmup_ratio=WARMUP_RATIO,\n    fp16=FP16,\n    batch_size=BATCH_SIZE,\n    logging_dir=LOGGING_DIR\n):\n    \"\"\"\n    Fine-tune a T5 model for recipe generation.\n    \"\"\"\n    print(f\"\\n--- Stage 3: Fine-tuning {base_model} ---\")\n    print(f\"Configuration: Epochs={num_train_epochs}, BatchSize={batch_size}, LR={learning_rate}, FP16={fp16}\")\n    print(f\"Output directory: {output_dir}\")\n    print(f\"Logs directory: {logging_dir}\")\n    print(f\"Ensure 'accelerate' is installed if FP16 is True.\")\n\n    # Load preprocessed dataset\n    try:\n        processed_dataset = load_from_disk(preprocessed_dataset_dir)\n        print(f\"Loaded preprocessed dataset from '{preprocessed_dataset_dir}'\")\n        # Ensure format is PyTorch tensors (might be redundant if set in preprocessing)\n        processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n        print(f\"Dataset contains {len(processed_dataset['train'])} training examples.\")\n        # Verify structure\n        print(f\"Train dataset columns: {processed_dataset['train'].column_names}\")\n        if not all(col in processed_dataset['train'].column_names for col in ['input_ids', 'attention_mask', 'labels']):\n             print(\"Error: Dataset missing required columns ('input_ids', 'attention_mask', 'labels'). Check preprocessing.\")\n             return None, None\n    except FileNotFoundError:\n        print(f\"Error: Preprocessed dataset directory '{preprocessed_dataset_dir}' not found.\")\n        print(\"Please ensure Stage 2 (preprocess_recipe_data) ran successfully.\")\n        return None, None\n    except Exception as e:\n        print(f\"Error loading or processing dataset: {e}\")\n        # Attempt to print details of the first element for debugging\n        try:\n            print(\"First element structure:\", processed_dataset[\"train\"][0])\n        except Exception as debug_e:\n            print(f\"Could not examine first element: {debug_e}\")\n        return None, None\n\n\n    # Load tokenizer (ensure it matches the one used for preprocessing)\n    tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n    special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n    num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n    if num_added_tokens > 0:\n        print(f\"Re-added {num_added_tokens} special tokens to tokenizer for safety.\")\n\n    # Load model\n    print(f\"Loading base model '{base_model}'...\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n    # Resize embeddings if new tokens were added\n    model.resize_token_embeddings(len(tokenizer))\n    print(f\"Model tokenizer vocabulary size: {len(tokenizer)}\")\n\n    # Verify model has trainable parameters\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if trainable_params == 0:\n         print(\"Warning: Model loaded has no trainable parameters!\")\n    else:\n         print(f\"Model has {trainable_params:,} trainable parameters.\")\n\n\n    # Set up training arguments\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,\n        # Evaluation and Saving Strategy\n        evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n        save_strategy=\"epoch\",       # Save at the end of each epoch\n        # Learning Rate and Optimization\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_ratio=warmup_ratio,\n        # Batch Size\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2, # Can often use larger batch size for eval\n        # Training Duration\n        num_train_epochs=num_train_epochs,\n        # Technical Details\n        fp16=fp16 and torch.cuda.is_available(), # Use FP16 only if flag is True AND cuda is available\n        gradient_accumulation_steps=2, # Accumulate gradients to simulate larger batch size if needed\n        # Checkpointing and Logging\n        save_total_limit=2, # Keep only the best and the latest checkpoints\n        load_best_model_at_end=True,\n        metric_for_best_model=\"loss\", # Use validation loss to determine the best model\n        greater_is_better=False,    # Lower loss is better\n        logging_dir=logging_dir,\n        logging_strategy=\"steps\",\n        logging_steps=50, # Log training loss every 50 steps\n        report_to=[\"tensorboard\"], # Report metrics to TensorBoard\n        # Generation arguments (used if predict_with_generate=True)\n        predict_with_generate=True,\n        generation_max_length=512,\n        generation_num_beams=4,\n    )\n\n    # Create data collator\n    # This pads batches dynamically - requires tokenizer to have pad_token set.\n    if tokenizer.pad_token is None:\n        print(\"Warning: Tokenizer does not have a pad token. Adding EOS as pad token.\")\n        tokenizer.pad_token = tokenizer.eos_token\n        model.config.pad_token_id = tokenizer.eos_token_id # Ensure model config knows pad token id\n\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        label_pad_token_id=-100, # Make sure padding in labels is ignored for loss\n        pad_to_multiple_of=8 if fp16 else None # Pad to multiple of 8 for efficiency with FP16\n    )\n\n    # Dummy compute_metrics - focusing on loss for now\n    # Replace with ROUGE, BLEU etc. later if needed\n    def compute_metrics(eval_preds):\n        # loss is calculated by trainer automatically\n        # Can add generation metrics later\n        return {\"placeholder_metric\": 0.0}\n\n    # Initialize Trainer\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_dataset[\"train\"],\n        eval_dataset=processed_dataset[\"validation\"],\n        tokenizer=tokenizer, # Pass tokenizer for padding and generation\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)] # Stop if eval loss doesn't improve significantly\n    )\n\n    # --- Start Training ---\n    print(\"\\nStarting fine-tuning training...\")\n    try:\n        train_result = trainer.train()\n        print(\"Training completed successfully!\")\n\n        # Save final model and tokenizer\n        trainer.save_model() # Saves the best model according to load_best_model_at_end=True\n        tokenizer.save_pretrained(output_dir)\n        print(f\"Best model and tokenizer saved to '{output_dir}'\")\n\n        # Log metrics\n        metrics = train_result.metrics\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n    except Exception as e:\n        print(f\"\\n!!! Training Error !!!\")\n        print(f\"Error type: {type(e)}\")\n        print(f\"Error details: {e}\")\n        # Simple debugging - check shapes of first batch\n        try:\n            print(\"\\nAttempting to inspect first training batch:\")\n            first_batch = next(iter(trainer.get_train_dataloader()))\n            print(f\"Keys in batch: {first_batch.keys()}\")\n            for key, tensor in first_batch.items():\n                print(f\"  {key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n        except Exception as debug_e:\n            print(f\"Could not inspect batch: {debug_e}\")\n        print(\"Training failed. Please check error message and dataset integrity.\")\n        return None, None # Indicate failure\n\n    # --- Evaluate on Test Set ---\n    print(\"\\nEvaluating final model on the test set...\")\n    try:\n        test_results = trainer.evaluate(eval_dataset=processed_dataset[\"test\"])\n        print(\"\\nTest Set Evaluation Results:\")\n        trainer.log_metrics(\"test\", test_results)\n        trainer.save_metrics(\"test\", test_results)\n        print(json.dumps(test_results, indent=2))\n    except Exception as e:\n        print(f\"Error during final evaluation on test set: {e}\")\n\n    print(\"\\n--- Fine-tuning Stage Completed ---\")\n    # Return the loaded best model and tokenizer\n    return model, tokenizer\n\n# --- Stage 4: Testing & Generation (Optional - Example) ---\n\nclass AllergenDetectionTester:\n    def __init__(self, model_path=OUTPUT_MODEL_DIR, base_model_fallback=BASE_MODEL):\n        \"\"\"Initialize the tester with model and post-processor.\"\"\"\n        print(f\"\\n--- Stage 4: Initializing AllergenDetectionTester ---\")\n        self.model = None\n        self.tokenizer = None\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.use_mock_generation = False\n\n        # Try loading fine-tuned model first\n        if model_path and os.path.isdir(model_path):\n             try:\n                 print(f\"Attempting to load fine-tuned model and tokenizer from '{model_path}'...\")\n                 self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                 self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n                 print(\"Successfully loaded fine-tuned model.\")\n                 self.model.to(self.device)\n             except Exception as e:\n                 print(f\"Warning: Failed to load model from '{model_path}'. Error: {e}\")\n                 self.model = None\n                 self.tokenizer = None\n        else:\n            print(f\"Fine-tuned model path '{model_path}' not found or invalid.\")\n\n        # Fallback to base model or mock generation\n        if self.model is None:\n             print(f\"Falling back to base model '{base_model_fallback}' for generation OR mock generation.\")\n             try:\n                 self.tokenizer = AutoTokenizer.from_pretrained(base_model_fallback, use_fast=True)\n                 # Optionally load base model for actual generation - can be slow on CPU\n                 # self.model = AutoModelForSeq2SeqLM.from_pretrained(base_model_fallback)\n                 # self.model.to(self.device)\n                 # print(\"Loaded base model for generation.\")\n                 print(\"Will use MOCK generation as base model loading is commented out / not requested.\")\n                 self.use_mock_generation = True # Set to use mock if base model isn't loaded\n             except Exception as e:\n                 print(f\"FATAL: Could not load base tokenizer '{base_model_fallback}'. Error: {e}\")\n                 raise # Can't proceed without a tokenizer\n\n        # Ensure special tokens are added (important for post-processor)\n        special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n        self.tokenizer.add_special_tokens(special_tokens)\n        if self.model and not self.use_mock_generation: # Resize embeddings if using loaded model\n              self.model.resize_token_embeddings(len(self.tokenizer))\n\n        # Initialize the post-processor WITH the loaded/fallback tokenizer\n        self.processor = RecipePostProcessor(self.tokenizer)\n\n        if self.model and not self.use_mock_generation:\n            self.model.eval() # Set model to evaluation mode\n\n        print(f\"Using device: {self.device}\")\n        print(\"Tester initialized.\")\n\n    def generate_recipe(self, ingredients):\n        \"\"\"Generate a recipe from ingredients using loaded model or mock.\"\"\"\n        if isinstance(ingredients, list):\n            ingredients_str = \", \".join(ingredients)\n        else:\n            ingredients_str = ingredients # Assume it's already a string\n\n        input_text = f\"items: {ingredients_str}\"\n\n        # Use mock generation if specified or if no model loaded\n        if self.use_mock_generation or not self.model:\n            print(f\"(Mock Generation for: {ingredients_str})\")\n            ingredients_list = [ing.strip() for ing in ingredients_str.split(\",\")]\n            # Make mock recipe a bit more realistic & include potential allergens from input\n            mock_title = f\"Mock {ingredients_list[0].capitalize()} Dish\"\n            mock_ingredients_str = \" <sep> \".join(ingredients_list)\n            mock_directions = f\"prepare the ingredients ({', '.join(ingredients_list[:2])}) <sep> mix everything together <sep> cook using {random.choice(['pan', 'oven', 'pot'])} <sep> season well <sep> serve\"\n            mock_recipe_text = (\n                f\"title: {mock_title} <section> \"\n                f\"ingredients: {mock_ingredients_str} <section> \"\n                f\"directions: {mock_directions}\"\n            )\n            return mock_recipe_text\n\n        # --- Actual Model Generation ---\n        print(f\"(Generating recipe with {self.model.name_or_path} for: {ingredients_str})\")\n        inputs = self.tokenizer(\n            input_text,\n            max_length=256,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Generation parameters\n        generation_kwargs = {\n            \"max_length\": 512,\n            \"min_length\": 50,\n            \"num_beams\": 4,\n            \"early_stopping\": True,\n            # For more creative/varied output:\n            # \"do_sample\": True,\n            # \"temperature\": 0.7,\n            # \"top_k\": 50,\n            # \"top_p\": 0.95,\n            # \"no_repeat_ngram_size\": 3\n        }\n\n        with torch.no_grad():\n            output_ids = self.model.generate(\n                inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                **generation_kwargs\n            )\n\n        # Decode - skip special tokens handled by postprocessor, clean others\n        # Set clean_up_tokenization_spaces=True for potentially better spacing.\n        generated_text = self.tokenizer.decode(\n            output_ids[0],\n            skip_special_tokens=False, # Let postprocessor handle <sep>/<section>\n            clean_up_tokenization_spaces=True\n            )\n\n        return generated_text\n\n\n    def test_allergen_combinations(self, use_actual_generation=True):\n        \"\"\"Test specific ingredient combinations for allergen detection.\"\"\"\n        test_combinations = [\n            # Potential Allergen Triggers\n            \"fish, parmesan cheese, pasta, garlic, olive oil\", # Fish + Cheese (Dairy)\n            \"shrimp, milk, noodles, ginger, soy sauce\",         # Shellfish + Milk (Dairy)\n            \"tuna, yogurt, bread, cucumber, dill\",           # Fish + Yogurt (Dairy)\n            \"salmon, curd, rice, lemon, spinach\",             # Fish + Curd (Dairy)\n            \"lobster, cream, butter, garlic, bread\",           # Shellfish + Cream (Dairy)\n            \"crab cakes (crab, breadcrumbs), mango salsa (mango, lime)\", # Shellfish + Mango\n            \"peanut butter cookies (peanuts, flour, egg)\",      # Peanuts + Flour (Gluten/Wheat)\n            \"strawberry chocolate tart (strawberry, chocolate, cream)\",# Strawberry + Chocolate\n\n            # Likely Non-Allergenic Controls (according to our list)\n            \"chicken breast, rice, broccoli, soy sauce, sesame oil\",\n            \"beef steak, potatoes, carrots, onion gravy\",\n            \"lentil soup (lentils, carrots, celery, onion, broth)\",\n            \"tofu stir-fry (tofu, bell peppers, onion, soy sauce)\"\n        ]\n\n        results = []\n        self.use_mock_generation = not use_actual_generation # Override based on arg\n\n        print(\"\\n======= TESTING ALLERGEN COMBINATIONS =======\\n\")\n        if not use_actual_generation and not self.model:\n             print(\"NOTE: Using MOCK generation because 'use_actual_generation' is False or no model was loaded.\")\n        elif not self.model:\n            print(\"NOTE: Using MOCK generation because no model could be loaded.\")\n\n        for ingredients in test_combinations:\n            print(f\"\\n--- Testing ingredients: {ingredients} ---\")\n\n            start_time = time.time()\n            # Generate a recipe (mock or real)\n            raw_recipe_text = self.generate_recipe(ingredients)\n            generation_time = time.time() - start_time\n            #print(f\"Raw output: {raw_recipe_text[:150]}...\") # Debug raw output\n\n            # Post-process the recipe\n            start_time = time.time()\n            processed_recipes = self.processor.postprocess_text([raw_recipe_text])\n            recipe_dict = processed_recipes[0] # Get the first (only) processed recipe\n            processing_time = time.time() - start_time\n\n            # Calculate quality score\n            quality_score = self.processor.evaluate_recipe_quality(recipe_dict)\n\n            # Format the recipe for display\n            formatted_recipe = self.processor.format_for_display(recipe_dict)\n\n            results.append({\n                \"input_ingredients\": ingredients,\n                \"generated_text_raw\": raw_recipe_text,\n                \"processed_recipe\": recipe_dict,\n                \"formatted_text\": formatted_recipe,\n                \"quality_score\": quality_score,\n                \"allergen_warnings\": recipe_dict.get(\"allergen_warnings\", []),\n                \"generation_time_sec\": round(generation_time, 2),\n                \"processing_time_sec\": round(processing_time, 2)\n            })\n\n            # Print the result for this combination\n            print(formatted_recipe)\n            print(f\"Quality Score: {quality_score}/100\")\n            print(f\"(Generation: {generation_time:.2f}s, Processing: {processing_time:.2f}s)\")\n            print(\"-\" * 60)\n\n\n        # --- Summary of Allergen Detections ---\n        print(\"\\n======= ALLERGEN DETECTION SUMMARY =======\\n\")\n        detected_count = 0\n        for i, result in enumerate(results):\n            warnings = result.get(\"allergen_warnings\", [])\n            print(f\"{i+1}. Ingredients: {result['input_ingredients']}\")\n            print(f\"   Detected Warnings: {len(warnings)}\")\n            if warnings:\n                detected_count += 1\n                for warning in warnings:\n                    print(f\"     - {warning['categories'][0].capitalize()} + {warning['categories'][1].capitalize()}: {warning['reason']}\")\n            print() # Newline for readability\n\n        safe_recipes_count = len(results) - detected_count\n        print(f\"Summary: Total Tested = {len(results)}, Recipes with Warnings = {detected_count}, Recipes without Warnings = {safe_recipes_count}\")\n\n        return results\n\n\n# --- Main Execution Block ---\n\nif __name__ == \"__main__\":\n    print(\"Starting Recipe Generation Pipeline...\")\n    print(f\"Using Base Model: {BASE_MODEL}\")\n    print(f\"Output Model Dir: {OUTPUT_MODEL_DIR}\")\n\n    # --- Step 1: Create Data (if needed) ---\n    # Check if raw data exists, otherwise create it\n    if not os.path.exists(RAW_DATASET_DIR):\n        print(f\"Raw dataset not found at '{RAW_DATASET_DIR}'. Running Stage 1...\")\n        create_recipe_dataset(sample_size=SAMPLE_SIZE, seed=SEED)\n    else:\n        print(f\"Raw dataset found at '{RAW_DATASET_DIR}'. Skipping Stage 1.\")\n\n    # --- Step 2: Preprocess Data (if needed) ---\n    # Check if preprocessed data exists, otherwise create it\n    if not os.path.exists(PREPROCESSED_DATA_DIR):\n         print(f\"Preprocessed dataset not found at '{PREPROCESSED_DATA_DIR}'. Running Stage 2...\")\n         processed_data, pp_tokenizer = preprocess_recipe_data()\n         if processed_data is None:\n             print(\"Preprocessing failed. Exiting.\")\n             exit()\n    else:\n        print(f\"Preprocessed dataset found at '{PREPROCESSED_DATA_DIR}'. Skipping Stage 2.\")\n        # We still need a tokenizer for later stages, load it from base\n        print(f\"Loading tokenizer '{BASE_MODEL}' for subsequent stages...\")\n        pp_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n        special_tokens = {\"additional_special_tokens\": [\"<sep>\", \"<section>\"]}\n        pp_tokenizer.add_special_tokens(special_tokens)\n\n\n    # --- Step 3: Fine-tune Model ---\n    # Set train=True to run fine-tuning, False to skip and proceed to testing\n    train_model = True\n    fine_tuned_model = None\n    fine_tuned_tokenizer = None\n\n    if train_model:\n        print(\"\\nProceeding to Stage 3: Fine-tuning...\")\n        # Pass directories explicitly\n        fine_tuned_model, fine_tuned_tokenizer = fine_tune_recipe_model(\n            base_model=BASE_MODEL,\n            output_dir=OUTPUT_MODEL_DIR,\n            preprocessed_dataset_dir=PREPROCESSED_DATA_DIR,\n            num_train_epochs=NUM_TRAIN_EPOCHS,\n            learning_rate=LEARNING_RATE,\n            weight_decay=WEIGHT_DECAY,\n            warmup_ratio=WARMUP_RATIO,\n            fp16=FP16,\n            batch_size=BATCH_SIZE,\n            logging_dir=LOGGING_DIR\n        )\n\n        if fine_tuned_model is None:\n            print(\"Fine-tuning failed or was skipped due to error. Exiting or proceeding without fine-tuned model...\")\n            # Decide whether to exit or allow testing with base/mock\n            # exit() # Uncomment to stop if training fails\n        else:\n             print(\"Fine-tuning seems successful.\")\n    else:\n        print(\"\\nSkipping Stage 3: Fine-tuning based on 'train_model' flag.\")\n        # Try to load if model exists from previous run\n        if os.path.exists(OUTPUT_MODEL_DIR):\n            print(f\"Attempting to load previously fine-tuned model from {OUTPUT_MODEL_DIR} for testing...\")\n            try:\n                fine_tuned_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_MODEL_DIR, use_fast=True)\n                # No need to load the full model here unless tester needs it directly passed\n                print(\"Loaded tokenizer for testing.\")\n            except Exception as e:\n                print(f\"Could not load tokenizer from {OUTPUT_MODEL_DIR}: {e}. Using base tokenizer.\")\n                fine_tuned_tokenizer = pp_tokenizer # Fallback to preprocessor tokenizer\n        else:\n             print(f\"No previously fine-tuned model found at {OUTPUT_MODEL_DIR}.\")\n             fine_tuned_tokenizer = pp_tokenizer # Fallback to preprocessor tokenizer\n\n\n    # --- Step 4: Test Generation and Allergen Detection ---\n    print(\"\\nProceeding to Stage 4: Testing Allergen Detection...\")\n    # Initialize tester - it will try to load from OUTPUT_MODEL_DIR\n    tester = AllergenDetectionTester(model_path=OUTPUT_MODEL_DIR, base_model_fallback=BASE_MODEL)\n\n    # Run the tests - specify whether to force mock generation or attempt actual generation\n    # Set use_actual_generation=True if you trained a model AND have enough compute/time\n    # Set use_actual_generation=False to always use MOCK generation for speed/debugging post-processing\n    use_actual_generation_flag = (fine_tuned_model is not None or os.path.exists(OUTPUT_MODEL_DIR)) and torch.cuda.is_available() # Example logic: Use real generation if trained/exists and GPU available\n    if not use_actual_generation_flag:\n         print(\"\\nNOTE: Will use MOCK recipe generation for allergen tests (either model not trained/found, or no GPU).\")\n\n    test_results = tester.test_allergen_combinations(use_actual_generation=use_actual_generation_flag)\n\n    # Optional: Save test results\n    results_file = os.path.join(OUTPUT_MODEL_DIR, \"allergen_test_results.json\")\n    try:\n        # Need to handle potential non-serializable items (like tensors if any slip through)\n        def default_serializer(obj):\n             if isinstance(obj, torch.Tensor):\n                 return obj.tolist() # Convert tensors to lists\n             # Add other types if needed\n             return f\"Object of type {type(obj).__name__} is not JSON serializable\"\n\n        with open(results_file, 'w') as f:\n            json.dump(test_results, f, indent=2, default=default_serializer)\n        print(f\"\\nSaved allergen test results to {results_file}\")\n    except Exception as e:\n        print(f\"\\nError saving test results to JSON: {e}\")\n\n\n    print(\"\\nRecipe Generation Pipeline Finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:53:47.270226Z","iopub.execute_input":"2025-04-06T13:53:47.270687Z","iopub.status.idle":"2025-04-06T14:07:39.836348Z","shell.execute_reply.started":"2025-04-06T13:53:47.270655Z","shell.execute_reply":"2025-04-06T14:07:39.835451Z"}},"outputs":[{"name":"stdout","text":"Starting Recipe Generation Pipeline...\nUsing Base Model: t5-base\nOutput Model Dir: ./recipe_model_finetuned\nRaw dataset not found at 'recipe_dataset'. Running Stage 1...\n\n--- Stage 1: Creating Synthetic Recipe Dataset ---\nGenerating 2000 synthetic recipes...\nSaved raw synthetic data to synthetic_recipe_data/synthetic_recipes.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c0d51b86c34069bef9abb0cebaf2d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6af8f9913b4aa8ae56cf1d7f3bb746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd6cffc534e4095bab8ff5ad214fd8a"}},"metadata":{}},{"name":"stdout","text":"Dataset created with 2000 recipes and saved to 'recipe_dataset'\nSplit sizes -> Train: 1600, Validation: 200, Test: 200\n\nExample recipes (first 3):\n\nInput: items: curd, eggs, lentils, tempeh\nOutput: title: Simple Lentils Stew <section> ingredients: curd <sep> eggs <sep> lentils <sep> tempeh <section> directions: Bring to a simmer, then reduce heat and cook for 26 minutes until lentils is cooked through. <sep> Preheat your oven to 219°C (341°F). <sep> In a bowl, combine curd and eggs. <sep> Add olive oil to a pot over medium heat. <sep> Add lentils, tempeh and cook for 7 minutes, stirring occasionally. <sep> Garnish with nuts and serve hot. <sep> Sauté onion until fragrant, about 3 minutes.\n\nInput: items: beef, cream, shrimp, lettuce, orange, chocolate, oats, lamb\nOutput: title: Steamed Lamb Casserole <section> ingredients: beef <sep> cream <sep> shrimp <sep> lettuce <sep> orange <sep> chocolate <sep> oats <sep> lamb <section> directions: Bring to a simmer, then reduce heat and cook for 26 minutes until lamb is cooked through. <sep> Season with salt, pepper, and other desired spices to taste. <sep> Stir in the remaining ingredients (chocolate, oats, lamb) and water. <sep> Prepare all ingredients: Chop vegetables, measure spices. <sep> Add olive oil to a pan over medium heat.\n\nInput: items: chocolate, mozzarella, milk, eggplant, rosemary, pepper, pork, tofu, brown sugar\nOutput: title: Simple Pepper Soup <section> ingredients: chocolate <sep> mozzarella <sep> milk <sep> eggplant <sep> rosemary <sep> pepper <sep> pork <sep> tofu <sep> brown sugar <section> directions: Prepare all ingredients: Chop vegetables, measure spices. <sep> Preheat your oven to 206°C (396°F). <sep> Bring to a simmer, then reduce heat and cook for 16 minutes until pepper is cooked through. <sep> Stir in the remaining ingredients (pepper, pork, tofu, brown sugar) and water. <sep> In a bowl, combine chocolate and mozzarella. <sep> Add butter to a pot over medium heat. <sep> Sauté onion until fragrant, about 5 minutes. <sep> Add milk, eggplant, rosemary and cook for 11 minutes, stirring occasionally.\nPreprocessed dataset not found at 'preprocessed_recipe_dataset'. Running Stage 2...\n\n--- Stage 2: Preprocessing Recipe Dataset ---\nLoaded raw dataset from 'recipe_dataset'\nLoading tokenizer 't5-base'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"671cd089af9e4c05b57af5df2cb35333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25e016a35a446028902d50b3084e424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba4a6404edb4f07a84c9b16100ff24d"}},"metadata":{}},{"name":"stdout","text":"Added 2 special tokens ('<sep>', '<section>') to the tokenizer.\nApplying preprocessing function...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a894740ed9425b94f02dfd791de04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fce10459ced46de871ad881c588b9cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39ff542d2ff4829b296ef07cefd1d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb193baa6754c79ac9f146d80c69aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c00dac672e4722aa6358532a46a310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4079706106cd47d1af6aa566dd5414e8"}},"metadata":{}},{"name":"stdout","text":"Preprocessed dataset saved to 'preprocessed_recipe_dataset'\n\nSample preprocessed example (first 10 tokens):\nInput IDs: [1173, 10, 5495, 26, 6, 5875, 6, 24026, 7, 6]\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLabels: [2233, 10, 9415, 301, 295, 1558, 3557, 210, 32101, 3018]\n\nProceeding to Stage 3: Fine-tuning...\n\n--- Stage 3: Fine-tuning t5-base ---\nConfiguration: Epochs=1, BatchSize=4, LR=5e-05, FP16=True\nOutput directory: ./recipe_model_finetuned\nLogs directory: ./recipe_logs\nEnsure 'accelerate' is installed if FP16 is True.\nLoaded preprocessed dataset from 'preprocessed_recipe_dataset'\nDataset contains 1600 training examples.\nTrain dataset columns: ['input_ids', 'attention_mask', 'labels']\nRe-added 2 special tokens to tokenizer for safety.\nLoading base model 't5-base'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094f148b13b8437f9516d82baecd2f03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d662831ceab446f88aa7b539a4e6bdfd"}},"metadata":{}},{"name":"stdout","text":"Model tokenizer vocabulary size: 32102\nModel has 222,883,584 trainable parameters.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-2-98fd39ae4d49>:629: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"\nStarting fine-tuning training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 08:13, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Placeholder Metric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.216900</td>\n      <td>0.961998</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Training completed successfully!\nBest model and tokenizer saved to './recipe_model_finetuned'\n***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =   453708GF\n  train_loss               =      3.191\n  train_runtime            = 0:08:15.46\n  train_samples_per_second =      3.229\n  train_steps_per_second   =      0.404\n\nEvaluating final model on the test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 04:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nTest Set Evaluation Results:\n***** test metrics *****\n  epoch                   =        1.0\n  eval_loss               =     0.9741\n  eval_placeholder_metric =        0.0\n  eval_runtime            = 0:04:33.44\n  eval_samples_per_second =      0.731\n  eval_steps_per_second   =      0.091\n{\n  \"eval_loss\": 0.9741456508636475,\n  \"eval_placeholder_metric\": 0.0,\n  \"eval_runtime\": 273.4498,\n  \"eval_samples_per_second\": 0.731,\n  \"eval_steps_per_second\": 0.091,\n  \"epoch\": 1.0\n}\n\n--- Fine-tuning Stage Completed ---\nFine-tuning seems successful.\n\nProceeding to Stage 4: Testing Allergen Detection...\n\n--- Stage 4: Initializing AllergenDetectionTester ---\nAttempting to load fine-tuned model and tokenizer from './recipe_model_finetuned'...\nSuccessfully loaded fine-tuned model.\nUsing device: cuda\nTester initialized.\n\n======= TESTING ALLERGEN COMBINATIONS =======\n\n\n--- Testing ingredients: fish, parmesan cheese, pasta, garlic, olive oil ---\n(Generating recipe with ./recipe_model_finetuned for: fish, parmesan cheese, pasta, garlic, olive oil)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.98s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: shrimp, milk, noodles, ginger, soy sauce ---\n(Generating recipe with ./recipe_model_finetuned for: shrimp, milk, noodles, ginger, soy sauce)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.54s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: tuna, yogurt, bread, cucumber, dill ---\n(Generating recipe with ./recipe_model_finetuned for: tuna, yogurt, bread, cucumber, dill)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.54s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: salmon, curd, rice, lemon, spinach ---\n(Generating recipe with ./recipe_model_finetuned for: salmon, curd, rice, lemon, spinach)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.22s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: lobster, cream, butter, garlic, bread ---\n(Generating recipe with ./recipe_model_finetuned for: lobster, cream, butter, garlic, bread)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.57s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: crab cakes (crab, breadcrumbs), mango salsa (mango, lime) ---\n(Generating recipe with ./recipe_model_finetuned for: crab cakes (crab, breadcrumbs), mango salsa (mango, lime))\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 1.31s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: peanut butter cookies (peanuts, flour, egg) ---\n(Generating recipe with ./recipe_model_finetuned for: peanut butter cookies (peanuts, flour, egg))\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.18s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: strawberry chocolate tart (strawberry, chocolate, cream) ---\n(Generating recipe with ./recipe_model_finetuned for: strawberry chocolate tart (strawberry, chocolate, cream))\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.29s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: chicken breast, rice, broccoli, soy sauce, sesame oil ---\n(Generating recipe with ./recipe_model_finetuned for: chicken breast, rice, broccoli, soy sauce, sesame oil)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 1.21s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: beef steak, potatoes, carrots, onion gravy ---\n(Generating recipe with ./recipe_model_finetuned for: beef steak, potatoes, carrots, onion gravy)\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.24s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: lentil soup (lentils, carrots, celery, onion, broth) ---\n(Generating recipe with ./recipe_model_finetuned for: lentil soup (lentils, carrots, celery, onion, broth))\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.18s, Processing: 0.00s)\n------------------------------------------------------------\n\n--- Testing ingredients: tofu stir-fry (tofu, bell peppers, onion, soy sauce) ---\n(Generating recipe with ./recipe_model_finetuned for: tofu stir-fry (tofu, bell peppers, onion, soy sauce))\n[TITLE]: Untitled Recipe\n\n[INGREDIENTS]:\n  (No ingredients listed)\n\n[DIRECTIONS]:\n  (No directions listed)\n\nQuality Score: 0/100\n(Generation: 2.20s, Processing: 0.00s)\n------------------------------------------------------------\n\n======= ALLERGEN DETECTION SUMMARY =======\n\n1. Ingredients: fish, parmesan cheese, pasta, garlic, olive oil\n   Detected Warnings: 0\n\n2. Ingredients: shrimp, milk, noodles, ginger, soy sauce\n   Detected Warnings: 0\n\n3. Ingredients: tuna, yogurt, bread, cucumber, dill\n   Detected Warnings: 0\n\n4. Ingredients: salmon, curd, rice, lemon, spinach\n   Detected Warnings: 0\n\n5. Ingredients: lobster, cream, butter, garlic, bread\n   Detected Warnings: 0\n\n6. Ingredients: crab cakes (crab, breadcrumbs), mango salsa (mango, lime)\n   Detected Warnings: 0\n\n7. Ingredients: peanut butter cookies (peanuts, flour, egg)\n   Detected Warnings: 0\n\n8. Ingredients: strawberry chocolate tart (strawberry, chocolate, cream)\n   Detected Warnings: 0\n\n9. Ingredients: chicken breast, rice, broccoli, soy sauce, sesame oil\n   Detected Warnings: 0\n\n10. Ingredients: beef steak, potatoes, carrots, onion gravy\n   Detected Warnings: 0\n\n11. Ingredients: lentil soup (lentils, carrots, celery, onion, broth)\n   Detected Warnings: 0\n\n12. Ingredients: tofu stir-fry (tofu, bell peppers, onion, soy sauce)\n   Detected Warnings: 0\n\nSummary: Total Tested = 12, Recipes with Warnings = 0, Recipes without Warnings = 12\n\nSaved allergen test results to ./recipe_model_finetuned/allergen_test_results.json\n\nRecipe Generation Pipeline Finished.\n","output_type":"stream"}],"execution_count":2}]}